#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¤šæºæ•°æ®ä¸‹è½½å™¨
æ”¯æŒä» Hugging Faceã€GitHubã€Google Drive ä¸‹è½½æ•°æ®
"""

import os
import re
import subprocess
import zipfile
from pathlib import Path
import logging
import requests
from huggingface_hub import snapshot_download

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class HuggingFaceDownloader:
    """
    Hugging Face æ•°æ®é›†ä¸‹è½½å™¨
    
    åŠŸèƒ½ï¼šä¸‹è½½æŒ‡å®šçš„ Hugging Face æ•°æ®é›†åˆ°æœ¬åœ°
    """
    
    def __init__(self, base_dir: str = None):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨
        
        å‚æ•°:
            base_dir (str, optional): æ•°æ®å­˜å‚¨åŸºç¡€ç›®å½•
        """
        if base_dir is None:
            self.base_dir = Path(__file__).parent.parent
        else:
            self.base_dir = Path(base_dir)
        
        self.datasets_dir = self.base_dir / "datasets"
        self.datasets_dir.mkdir(parents=True, exist_ok=True)
    
    def download(self, repo_id: str, force_download: bool = False, ignore_patterns: list = None) -> bool:
        """
        ä¸‹è½½æŒ‡å®šçš„ Hugging Face æ•°æ®é›†
        
        å‚æ•°:
            repo_id (str): Hugging Face ä»“åº“IDï¼Œä¾‹å¦‚ 'zjunlp/Mol-Instructions'
            force_download (bool): æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
            ignore_patterns (list): è¦å¿½ç•¥çš„æ–‡ä»¶æ¨¡å¼åˆ—è¡¨ï¼Œä¾‹å¦‚ ['dna/dna_seq.txt', '*.log']
        
        è¿”å›:
            bool: ä¸‹è½½æˆåŠŸè¿”å› True
        """
        # ä» repo_id æå–æ•°æ®é›†åç§°ä½œä¸ºæœ¬åœ°æ–‡ä»¶å¤¹å
        dataset_name = repo_id.replace('/', '_').replace('-', '_').lower()
        local_dir = self.datasets_dir / dataset_name
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if not force_download and self._check_exists(local_dir):
            logger.info(f"æ•°æ®é›†å·²å­˜åœ¨: {local_dir}")
            return True
        
        try:
            logger.info(f"å¼€å§‹ä¸‹è½½æ•°æ®é›†: {repo_id}")
            
            # å‡†å¤‡ä¸‹è½½å‚æ•°
            download_kwargs = {
                "repo_id": repo_id,
                "repo_type": "dataset",
                "local_dir": str(local_dir),
                "local_dir_use_symlinks": False
            }
            
            # å¦‚æœæœ‰å¿½ç•¥æ¨¡å¼ï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
            if ignore_patterns:
                download_kwargs["ignore_patterns"] = ignore_patterns
                logger.info(f"å¿½ç•¥æ–‡ä»¶æ¨¡å¼: {ignore_patterns}")
            
            # ä¸‹è½½æ•°æ®é›†
            downloaded_path = snapshot_download(**download_kwargs)
            
            logger.info(f"ä¸‹è½½å®Œæˆ: {downloaded_path}")
            return True
            
        except Exception as e:
            logger.error(f"ä¸‹è½½å¤±è´¥: {str(e)}")
            return False
    
    def _check_exists(self, local_dir: Path) -> bool:
        """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨"""
        return local_dir.exists() and any(local_dir.iterdir())
    
    def get_dataset_path(self, repo_id: str) -> Path:
        """è·å–æ•°æ®é›†æœ¬åœ°è·¯å¾„"""
        dataset_name = repo_id.replace('/', '_').replace('-', '_').lower()
        return self.datasets_dir / dataset_name
    
    def download_with_exclusions(self, repo_id: str, exclude_files: list, force_download: bool = False) -> bool:
        """
        ä¸‹è½½æ•°æ®é›†ä½†æ’é™¤æŒ‡å®šæ–‡ä»¶
        
        å‚æ•°:
            repo_id (str): Hugging Face ä»“åº“ID
            exclude_files (list): è¦æ’é™¤çš„æ–‡ä»¶åˆ—è¡¨ï¼Œä¾‹å¦‚ ['dna/dna_seq.txt']
            force_download (bool): æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
        
        è¿”å›:
            bool: ä¸‹è½½æˆåŠŸè¿”å› True
        """
        return self.download(repo_id, force_download, ignore_patterns=exclude_files)


class GitHubDownloader:
    """
    GitHub ä»“åº“ä¸‹è½½å™¨
    
    åŠŸèƒ½ï¼šä¸‹è½½æŒ‡å®šçš„ GitHub ä»“åº“åˆ°æœ¬åœ°
    """
    
    def __init__(self, base_dir: str = None):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨
        
        å‚æ•°:
            base_dir (str, optional): æ•°æ®å­˜å‚¨åŸºç¡€ç›®å½•
        """
        if base_dir is None:
            self.base_dir = Path(__file__).parent.parent
        else:
            self.base_dir = Path(base_dir)
        
        self.datasets_dir = self.base_dir / "datasets"
        self.datasets_dir.mkdir(parents=True, exist_ok=True)
    
    def download(self, repo_url: str, force_download: bool = False) -> bool:
        """
        ä¸‹è½½æŒ‡å®šçš„ GitHub ä»“åº“
        
        å‚æ•°:
            repo_url (str): GitHub ä»“åº“URLï¼Œä¾‹å¦‚ 'https://github.com/hhnqqq/Biology-Instructions'
            force_download (bool): æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
        
        è¿”å›:
            bool: ä¸‹è½½æˆåŠŸè¿”å› True
        """
        # ä» URL æå–ä»“åº“åä½œä¸ºæœ¬åœ°æ–‡ä»¶å¤¹å
        repo_name = repo_url.split('/')[-1].lower().replace('-', '_')
        local_dir = self.datasets_dir / repo_name
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if not force_download and self._check_exists(local_dir):
            logger.info(f"GitHub ä»“åº“å·²å­˜åœ¨: {local_dir}")
            return True
        
        try:
            logger.info(f"å¼€å§‹ä¸‹è½½ GitHub ä»“åº“: {repo_url}")
            
            # ä½¿ç”¨ git clone ä¸‹è½½ä»“åº“
            cmd = ['git', 'clone', repo_url, str(local_dir)]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info(f"ä¸‹è½½å®Œæˆ: {local_dir}")
                return True
            else:
                logger.error(f"Git clone å¤±è´¥: {result.stderr}")
                return False
            
        except Exception as e:
            logger.error(f"ä¸‹è½½å¤±è´¥: {str(e)}")
            return False
    
    def _check_exists(self, local_dir: Path) -> bool:
        """æ£€æŸ¥ä»“åº“æ˜¯å¦å·²å­˜åœ¨"""
        return local_dir.exists() and any(local_dir.iterdir())
    
    def get_dataset_path(self, repo_url: str) -> Path:
        """è·å–ä»“åº“æœ¬åœ°è·¯å¾„"""
        repo_name = repo_url.split('/')[-1].lower().replace('-', '_')
        return self.datasets_dir / repo_name


class HttpFileDownloader:
    """
    HTTP æ–‡ä»¶ä¸‹è½½å™¨
    
    åŠŸèƒ½ï¼šä»ä»»æ„ HTTP/HTTPS é“¾æ¥ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°
    """
    
    def __init__(self, base_dir: str = None):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨
        
        å‚æ•°:
            base_dir (str, optional): æ•°æ®å­˜å‚¨åŸºç¡€ç›®å½•
        """
        if base_dir is None:
            self.base_dir = Path(__file__).parent.parent
        else:
            self.base_dir = Path(base_dir)
        
        self.datasets_dir = self.base_dir / "datasets"
        self.datasets_dir.mkdir(parents=True, exist_ok=True)
    
    def download(self, download_url: str, force_download: bool = False) -> bool:
        """
        ä»æŒ‡å®šçš„ HTTP/HTTPS é“¾æ¥ä¸‹è½½æ–‡ä»¶
        
        å‚æ•°:
            download_url (str): æ–‡ä»¶ä¸‹è½½é“¾æ¥
            force_download (bool): æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
        
        è¿”å›:
            bool: ä¸‹è½½æˆåŠŸè¿”å› True
        """
        # ä½¿ç”¨ç»Ÿä¸€çš„è·¯å¾„ç”Ÿæˆé€»è¾‘
        local_dir = self.get_dataset_path(download_url)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if not force_download and self._check_exists(local_dir):
            logger.info(f"HTTP æ–‡ä»¶å·²å­˜åœ¨: {local_dir}")
            return True
        
        try:
            logger.info(f"å¼€å§‹ä¸‹è½½æ–‡ä»¶: {download_url}")
            
            # åˆ›å»ºæœ¬åœ°ç›®å½•
            local_dir.mkdir(exist_ok=True)
            
            with requests.Session() as session:
                # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                
                response = session.get(download_url, headers=headers, stream=True)
                
                if response.status_code == 200:
                    # ç¡®å®šæ–‡ä»¶å
                    filename = self._get_filename_from_response(response, download_url)
                    file_path = local_dir / filename
                    
                    logger.info(f"ä¿å­˜æ–‡ä»¶: {filename}")
                    
                    # ä¿å­˜æ–‡ä»¶
                    total_size = 0
                    with open(file_path, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                total_size += len(chunk)
                                
                                # æ¯ä¸‹è½½10MBæ‰“å°ä¸€æ¬¡è¿›åº¦
                                if total_size % (10 * 1024 * 1024) == 0:
                                    logger.info(f"å·²ä¸‹è½½: {total_size / (1024*1024):.1f} MB")
                    
                    logger.info(f"æ–‡ä»¶ä¸‹è½½å®Œæˆï¼Œæ€»å¤§å°: {total_size / (1024*1024):.1f} MB")
                    
                    # å¦‚æœæ˜¯zipæ–‡ä»¶ï¼Œè§£å‹
                    if filename.lower().endswith('.zip'):
                        logger.info("è§£å‹ZIPæ–‡ä»¶...")
                        with zipfile.ZipFile(file_path, 'r') as zip_ref:
                            zip_ref.extractall(local_dir)
                        os.remove(file_path)  # åˆ é™¤zipæ–‡ä»¶
                        logger.info("ZIPæ–‡ä»¶è§£å‹å®Œæˆ")
                    
                    logger.info(f"ä¸‹è½½å®Œæˆ: {local_dir}")
                    return True
                else:
                    logger.error(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    return False
            
        except Exception as e:
            logger.error(f"ä¸‹è½½å¤±è´¥: {str(e)}")
            return False
    

    
    def _get_filename_from_response(self, response, original_url: str) -> str:
        """ä»å“åº”å¤´ä¸­è·å–æ–‡ä»¶åï¼Œç¡®ä¿åŒä¸€é“¾æ¥è¿”å›å›ºå®šæ–‡ä»¶å"""
        # é¦–å…ˆå°è¯•ä»Content-Dispositionå¤´è·å–
        content_disposition = response.headers.get('content-disposition', '')
        if 'filename=' in content_disposition:
            filename = content_disposition.split('filename=')[1].strip('"').strip("'")
            return filename
        
        # å°è¯•ä»åŸå§‹URLä¸­æå–Google Driveæ–‡ä»¶IDï¼ˆå¦‚æœæ˜¯Google Driveé“¾æ¥ï¼‰
        url_to_check = original_url
        if hasattr(response, 'url') and response.url:
            url_to_check = response.url
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯Google Driveé“¾æ¥ï¼Œæå–æ–‡ä»¶ID
        if 'drive.google' in url_to_check or 'drive.usercontent.google' in url_to_check:
            file_id_match = re.search(r'id=([a-zA-Z0-9_-]+)', url_to_check)
            if file_id_match:
                file_id = file_id_match.group(1)
                return f"stage2_train.jsonl"  # æ ¹æ®å®é™…æƒ…å†µè¿”å›å·²çŸ¥çš„æ–‡ä»¶å
        
        # å°è¯•ä»URLè·¯å¾„ä¸­è·å–æ–‡ä»¶åï¼ˆå»é™¤åŠ¨æ€å‚æ•°ï¼‰
        if url_to_check:
            url_parts = url_to_check.split('/')
            if url_parts:
                potential_filename = url_parts[-1].split('?')[0]
                if '.' in potential_filename and len(potential_filename) < 100:  # é¿å…è¿‡é•¿çš„å‚æ•°
                    return potential_filename
        
        # é»˜è®¤æ–‡ä»¶åä½¿ç”¨URLå“ˆå¸Œ
        clean_url = re.sub(r'[&?](uuid|at|authuser|confirm)=[^&]*', '', original_url)
        url_hash = str(abs(hash(clean_url)))[:10]
        return f"http_file_{url_hash}"
    
    def _check_exists(self, local_dir: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨"""
        return local_dir.exists() and any(local_dir.iterdir())
    
    def get_dataset_path(self, download_url: str) -> Path:
        """è·å–æ–‡ä»¶æœ¬åœ°è·¯å¾„ï¼Œç¡®ä¿åŒä¸€é“¾æ¥è¿”å›å›ºå®šè·¯å¾„"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯Google Driveé“¾æ¥ï¼Œæå–æ–‡ä»¶ID
        if 'drive.google' in download_url or 'drive.usercontent.google' in download_url:
            file_id_match = re.search(r'id=([a-zA-Z0-9_-]+)', download_url)
            if file_id_match:
                file_id = file_id_match.group(1)
                return self.datasets_dir / f"gdrive_{file_id}"
        
        # å¯¹äºå…¶ä»–é“¾æ¥ï¼Œä½¿ç”¨URLå“ˆå¸Œï¼ˆå»é™¤åŠ¨æ€å‚æ•°åè®¡ç®—ï¼‰
        # å»é™¤å¸¸è§çš„åŠ¨æ€å‚æ•°
        clean_url = re.sub(r'[&?](uuid|at|authuser|confirm)=[^&]*', '', download_url)
        url_hash = str(abs(hash(clean_url)))[:10]
        return self.datasets_dir / f"http_{url_hash}"


def main():
    """
    ä¸»å‡½æ•° - ä¸‹è½½æ‰€æœ‰ç±»å‹çš„æ•°æ®é›†
    """
    print("ğŸš€ å¼€å§‹ä¸‹è½½å¤šæºæ•°æ®é›†...")
    
    # Hugging Face æ•°æ®é›†é…ç½® (repo_id, exclude_files)
    hf_datasets = [
        ('zjunlp/Mol-Instructions', None),
        ('PharMolix/UniProtQA', None), 
        ('EMCarrami/Pika-DS', None),
        ('InstaDeepAI/ChatNT_training_data', None),
        ('dnagpt/llama-gene-train-data', ['dna/dna_seq.txt', 'protein/protein_seq.txt'])  # ç¤ºä¾‹ï¼šæ’é™¤å¤§æ–‡ä»¶
    ]
    
    # GitHub ä»“åº“
    github_repos = [
        'https://github.com/hhnqqq/Biology-Instructions'
    ]
    
    # HTTP æ–‡ä»¶ä¸‹è½½é“¾æ¥
    http_files = [
        'https://drive.usercontent.google.com/download?id=1OC3VpPKSQ0VHd9ZeZhnxI8EA2wTdrBg5&export=download&authuser=0&confirm=t&uuid=e785cd67-94d7-46b6-a1b4-14f643624b7d&at=AN8xHor66TqLKD-p6ddJcODmObWc%3A1756722563687'
    ]
    
    total_success = 0
    total_count = len(hf_datasets) + len(github_repos) + len(http_files)
    
    # ä¸‹è½½ Hugging Face æ•°æ®é›†
    print("\nğŸ“¥ ä¸‹è½½ Hugging Face æ•°æ®é›†...")
    hf_downloader = HuggingFaceDownloader()
    for i, (repo_id, exclude_files) in enumerate(hf_datasets, 1):
        print(f"[{i}/{len(hf_datasets)}] {repo_id}")
        if exclude_files:
            print(f"    æ’é™¤æ–‡ä»¶: {exclude_files}")
        
        if hf_downloader.download(repo_id, ignore_patterns=exclude_files):
            print(f"âœ… æˆåŠŸ")
            total_success += 1
        else:
            print(f"âŒ å¤±è´¥")
    
    # ä¸‹è½½ GitHub ä»“åº“
    print("\nğŸ“¥ ä¸‹è½½ GitHub ä»“åº“...")
    gh_downloader = GitHubDownloader()
    for i, repo_url in enumerate(github_repos, 1):
        print(f"[{i}/{len(github_repos)}] {repo_url}")
        if gh_downloader.download(repo_url):
            print(f"âœ… æˆåŠŸ")
            total_success += 1
        else:
            print(f"âŒ å¤±è´¥")
    
    # ä¸‹è½½ HTTP æ–‡ä»¶
    print("\nğŸ“¥ ä¸‹è½½ HTTP æ–‡ä»¶...")
    http_downloader = HttpFileDownloader()
    for i, download_url in enumerate(http_files, 1):
        print(f"[{i}/{len(http_files)}] {download_url[:80]}...")
        if http_downloader.download(download_url):
            print(f"âœ… æˆåŠŸ")
            total_success += 1
        else:
            print(f"âŒ å¤±è´¥")
    
    print(f"\nğŸ‰ å®Œæˆï¼æˆåŠŸä¸‹è½½ {total_success}/{total_count} ä¸ªæ•°æ®é›†")
    print(f"ğŸ“ å­˜å‚¨ä½ç½®: {hf_downloader.datasets_dir}")


if __name__ == "__main__":
    main()