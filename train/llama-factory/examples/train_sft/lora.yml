# === model ===
model_name_or_path: /root/autodl-fs/wzq/models/models--SciReason--SciReasoner-8B/snapshots/772c4adaf43c750db5ef04d6f567148ca3daf7b0
trust_remote_code: true
template: qwen

# === data ===
dataset_dir: /root/autodl-fs/wzq/BiomniGEM/train/llama-factory/data
dataset: synbiocot_chat
eval_dataset: synbiocot_val          # <- 原 val_dataset
cutoff_len: 4096
packing: false
preprocessing_num_workers: 8         # <- 原 dataset_process_num

# === train ===
stage: sft
finetuning_type: lora
lora_target: q_proj,v_proj,k_proj,o_proj,up_proj,down_proj,gate_proj
lora_rank: 128
lora_alpha: 16
lora_dropout: 0.05

per_device_train_batch_size: 16
per_device_eval_batch_size: 16       # <- 原 eval_batch_size
gradient_accumulation_steps: 128
learning_rate: 1.5e-4
weight_decay: 0.05
num_train_epochs: 3
lr_scheduler_type: cosine
warmup_ratio: 0.05
bf16: true
gradient_checkpointing: true
max_grad_norm: 1.0

logging_steps: 10
logging_first_step: true
save_steps: 200
save_total_limit: 5
eval_strategy: steps
eval_steps: 200
ddp_timeout: 180000
report_to: none
dataloader_num_workers: 8

output_dir: /root/autodl-fs/wzq/BiomniGEM/experiment/sft-lora
