#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ==============================
# å¯é…ç½®è¶…å‚æ•°ï¼ˆé›†ä¸­ä¿®æ”¹è¿™é‡Œï¼‰
# ==============================
MODEL_PATH = "/root/autodl-fs/wzq/models/models--SciReason--SciReasoner-8B/snapshots/772c4adaf43c750db5ef04d6f567148ca3daf7b0"
TRAIN_DATASET_PATH = "/root/autodl-fs/wzq/datasets/SynBioCoT/json/train.json"
VALIDATE_DATASET_PATH = "/root/autodl-fs/wzq/datasets/SynBioCoT/json/cell_validation.json"  # ä»…ä¿ç•™å˜é‡ï¼Œä¸å†ä½¿ç”¨
OUTPUT_DIR = "./outputs/biomnigem-sft-lora"

# LoRA
LORA_R = 128
LORA_ALPHA = 256
LORA_DROPOUT = 0.1
# æ–°ç‰ˆ peft æ”¯æŒ "all-linear"ï¼›è‹¥æ˜¯æ—§ç‰ˆ peftï¼Œè¯·æ”¹æˆé€—å·åˆ†éš”åˆ—è¡¨ï¼š
# "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj"
LORA_TARGET_MODULES = "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj"

# è®­ç»ƒ
NUM_TRAIN_EPOCHS = 3
PER_DEVICE_TRAIN_BATCH_SIZE = 2
PER_DEVICE_EVAL_BATCH_SIZE = 1  # å·²ä¸ç”¨ï¼Œä½†ä¿ç•™å˜é‡
GRADIENT_ACCUMULATION_STEPS = 4
LEARNING_RATE = 1e-4
WEIGHT_DECAY = 0.05
WARMUP_RATIO = 0.1
MAX_SEQ_LENGTH = 4096  # ä¸æ•°æ®æ„å»ºä¸€è‡´

# æ—¥å¿—/ä¿å­˜ï¼ˆè¯„ä¼°è¢«ç¦ç”¨ï¼‰
LOGGING_STEPS = 10
EVAL_STEPS = 1000000  # å·²æ— æ•ˆ
SAVE_STEPS = 100
SAVE_TOTAL_LIMIT = 20
LR_SCHEDULER_TYPE = "cosine"

# ç”Ÿæˆè¯„ä¼°ï¼ˆå·²ç¦ç”¨ï¼Œä¸å†ä½¿ç”¨ï¼‰
PREDICT_WITH_GENERATE = False
GENERATION_MAX_LENGTH = 1024
GENERATION_NUM_BEAMS = 1

# DDP / æ··ç²¾åº¦ / å…¶ä»–
DDP_FIND_UNUSED_PARAMETERS = False
GRADIENT_CHECKPOINTING = False
SEED = 42
# æ··ç²¾åº¦ï¼šNone=è‡ªåŠ¨é€‰æ‹©ï¼›ä¹Ÿå¯æ˜¾å¼ True/False å¼ºåˆ¶
USE_BF16 = True
USE_FP16 = None

# ==============================
# ä¾èµ– & å·¥å…·
# ==============================
import os
from typing import List
import torch
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer, SFTConfig
from tools import (
    set_seed,
    load_tokenizer,
    get_train_validate_dataset,  # ä»å¤ç”¨æ­¤å‡½æ•°ï¼Œä½†åªå–è®­ç»ƒæ•°æ®
    load_model,
    get_data_collator,
    # validate_one,  # ä¸å†éœ€è¦
)

def _auto_mp():
    """æ ¹æ®ç¡¬ä»¶ä¸æ˜¾å¼é…ç½®å†³å®š bf16/fp16ï¼ˆä¸æ”¹å˜ä½ è®¾ç½®çš„è¯­ä¹‰ï¼‰"""
    bf16_supported = torch.cuda.is_available() and getattr(torch.cuda, "is_bf16_supported", lambda: False)()
    if USE_BF16 is None and USE_FP16 is None:
        use_bf16 = bf16_supported
        use_fp16 = (torch.cuda.is_available() and not use_bf16)
    else:
        use_bf16 = bool(USE_BF16) if USE_BF16 is not None else False
        use_fp16 = bool(USE_FP16) if USE_FP16 is not None else (torch.cuda.is_available() and not use_bf16)
    return use_bf16, use_fp16

def _parse_target_modules(s: str):
    if isinstance(s, str) and s != "all-linear":
        return [x.strip() for x in s.split(",") if x.strip()]
    return s  # "all-linear" æˆ–å·²æ˜¯åˆ—è¡¨

def _maybe_set_mem_env():
    # é™ä½æ˜¾å­˜ç¢ç‰‡é£é™©ï¼ˆä¸æ”¹å˜è®­ç»ƒé€»è¾‘/è¶…å‚ï¼‰
    os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")
    try:
        # ä¸æ”¹æ•°å€¼è¯­ä¹‰ï¼Œæ”¾å®½ TF32ï¼ˆA100/H800 matmulï¼‰ï¼Œä¸ bf16 å…±å­˜
        torch.backends.cuda.matmul.allow_tf32 = True
        # åœ¨è¾ƒæ–° PyTorch ä¸­æé«˜ float32 matmul ç²¾åº¦è®¾ç½®ï¼ˆå¯¹ç¨³å®šæ€§æ›´å‹å¥½ï¼‰
        if hasattr(torch, "set_float32_matmul_precision"):
            torch.set_float32_matmul_precision("high")
    except Exception:
        pass

def _maybe_enable_flash_attn(model):
    # ä¸å¼ºåˆ¶ï¼›è‹¥ç¯å¢ƒå…·å¤‡ flash-attn2ï¼Œåˆ™å¼€å¯ï¼›å¦åˆ™ç»´æŒé»˜è®¤å®ç°
    try:
        # transformers>=4.40 æ”¯æŒè¯¥å­—æ®µï¼›è‹¥ä¸æ”¯æŒä¼šæŠ›å¼‚å¸¸
        model.config.attn_implementation = "flash_attention_2"
    except Exception:
        pass

def build_trainer():
    # 0) å†…å­˜/åç«¯å°ä¼˜åŒ–ï¼ˆä¸æ”¹å˜è®­ç»ƒè¯­ä¹‰ï¼‰
    _maybe_set_mem_env()

    # 1) éšæœºç§å­
    set_seed(SEED)
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # 2) Tokenizer
    tokenizer = load_tokenizer(MODEL_PATH, padding_side="right")
    # å¯¹é½ pad/eosï¼Œé¿å… padding è­¦å‘Šä¸æ— è°“çš„å¼ é‡æ‰©å¼ 
    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:
        tokenizer.pad_token = tokenizer.eos_token

    # 3) æ•°æ®é›† â€”â€” åªå–è®­ç»ƒæ•°æ®ï¼›éªŒè¯é›†/ç­”æ¡ˆè¢«å¿½ç•¥
    train_text, _, _ = get_train_validate_dataset(
        TRAIN_DATASET_PATH, VALIDATE_DATASET_PATH, tokenizer=tokenizer, seed=SEED
    )
    # 4) Collatorï¼ˆä»…å¯¹ assistant è®¡ç®—æŸå¤±ï¼‰
    collator = get_data_collator(tokenizer)

    # 5) æ¨¡å‹ + LoRA
    model = load_model(MODEL_PATH)
    _maybe_enable_flash_attn(model)

    target_modules = _parse_target_modules(LORA_TARGET_MODULES)
    lora_cfg = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=target_modules,
    )
    model = get_peft_model(model, lora_cfg)

    # 6) æ··ç²¾åº¦
    use_bf16, use_fp16 = _auto_mp()
    try:
        if use_bf16:
            model.to(dtype=torch.bfloat16)
    except Exception:
        # ä¸ªåˆ«åç«¯/LoRA åŒ…è£…ä¸æ”¯æŒæ•´æ¨¡è½¬æ¢åˆ™å¿½ç•¥
        pass

    # ä¾¿äºï¼ˆå¯èƒ½çš„ï¼‰GCï¼›æ˜¯å¦å¼€å¯ä»ç”± GRADIENT_CHECKPOINTING æ§åˆ¶
    if hasattr(model, "config"):
        model.config.use_cache = False

    if GRADIENT_CHECKPOINTING and hasattr(model, "gradient_checkpointing_enable"):
        model.gradient_checkpointing_enable()

    # 7) SFT é…ç½®ï¼šå®Œå…¨å…³é—­è¯„ä¼°ï¼Œåªè®­ç»ƒï¼›æŒ‰ SAVE_STEPS ä¿å­˜
    sft_args = SFTConfig(
        output_dir=OUTPUT_DIR,
        num_train_epochs=NUM_TRAIN_EPOCHS,
        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,
        # ä»…è®­ç»ƒï¼Œä¸éœ€è¦ eval batch
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        warmup_ratio=WARMUP_RATIO,
        logging_steps=LOGGING_STEPS,

        # ğŸš« å…³é—­è¯„ä¼°
        do_eval=False,
        eval_strategy="no",   # å…³é”®ï¼šä¸è¯„ä¼°
        # eval_steps=EVAL_STEPS,    # ä¸éœ€è¦
        # predict_with_generate=False,

        # âœ… åªæŒ‰æ­¥ä¿å­˜ checkpoint
        save_strategy="steps",
        save_steps=SAVE_STEPS,      # 100
        save_total_limit=SAVE_TOTAL_LIMIT,

        # ğŸš« æ— è¯„ä¼°å°±ä¸è¦åŠ è½½æœ€ä¼˜æ¨¡å‹
        load_best_model_at_end=False,
        # metric_for_best_model="acc",   # ä¸éœ€è¦
        # greater_is_better=True,        # ä¸éœ€è¦

        lr_scheduler_type=LR_SCHEDULER_TYPE,
        gradient_checkpointing=GRADIENT_CHECKPOINTING,
        ddp_find_unused_parameters=DDP_FIND_UNUSED_PARAMETERS,
        bf16=use_bf16,
        fp16=(use_fp16 and not use_bf16),
        max_seq_length=MAX_SEQ_LENGTH,
        packing=False,
        dataset_text_field="text",
        report_to="none",
    )

    # 8) ç»„è£… Trainerï¼ˆä¸ä¼  eval_dataset / compute_metricsï¼‰
    trainer = SFTTrainer(
        model=model,
        args=sft_args,
        train_dataset=train_text,
        tokenizer=tokenizer,
        data_collator=collator,
    )

    # å¯è®­ç»ƒå‚æ•°é‡ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    if int(os.environ.get("RANK", "0")) == 0:
        try:
            trainable, total = 0, 0
            for _, p in model.named_parameters():
                n = p.numel()
                total += n
                if p.requires_grad:
                    trainable += n
            print(f"Trainable params: {trainable/1e6:.2f}M / Total: {total/1e6:.2f}M "
                  f"({100*trainable/max(1,total):.2f}%)")
            print(f"Train samples: {len(train_text)}")
        except Exception:
            pass

    return trainer

def main():
    trainer = build_trainer()
    trainer.train()
    if trainer.is_world_process_zero():
        trainer.model.save_pretrained(OUTPUT_DIR)   # ä¿å­˜ LoRA é€‚é…å™¨
        trainer.tokenizer.save_pretrained(OUTPUT_DIR)
        print(f"âœ… Done. LoRA adapters & tokenizer saved to: {OUTPUT_DIR}")

if __name__ == "__main__":
    main()
