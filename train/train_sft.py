#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
BiomniGEM SFT è®­ç»ƒï¼ˆé‡æ„å¹¶è¡Œç¨³å¥ç‰ˆï¼‰
------------------------------------------------------------
æ”¹åŠ¨è¦ç‚¹ï¼ˆæ¥å£ä¸ä¸šåŠ¡é€»è¾‘ä¿æŒä¸å˜ï¼‰ï¼š
- ä½¿ç”¨ HuggingFace Trainerï¼ˆAccelerate åç«¯ï¼‰ç»Ÿä¸€åˆ†å¸ƒå¼ï¼ˆDDP/FSDP/DeepSpeedï¼‰ï¼Œç§»é™¤å¤–å±‚ Accelerator.prepare ä¸ TRL SFTTrainer çš„å åŠ å†²çª
- è®­ç»ƒæ•°æ®çº¯ text é¢„åˆ†è¯ + DataCollatorForLanguageModeling(mlm=False) åŠ¨æ€å¡«å……ï¼Œé¿å… messages/pad æŠ¥é”™
- éªŒè¯æ—¶ä¸´æ—¶å·¦å¡«å……ã€æ‰¹é‡ generateï¼Œç»´æŒåŸæœ‰ <answer> è§£æã€coverage/accuracy ç»Ÿè®¡ã€best checkpoint é€‰æ‹©ä¸æ¸…ç†
- LoRA æ³¨å…¥ç­–ç•¥ä¸åŸè„šæœ¬ä¿æŒä¸€è‡´ï¼ˆè‡ªåŠ¨æ¢æµ‹ q/k/v/o ä¸ up/down/gateï¼‰
"""

import os
import re
import json
import csv
import sys
import time
import shutil
import logging
import random
import signal
import argparse
from typing import Dict, Any, List, Optional, Tuple

import torch
from datasets import load_from_disk, Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    TrainerCallback,
    set_seed,
    DataCollatorForLanguageModeling,
)
from transformers.trainer_utils import is_main_process
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from tqdm import tqdm

# =========================
# å¸¸é‡ / å·¥å…·
# =========================

ANSWER_RE = re.compile(r"<answer>(.*?)</answer>", flags=re.S)  # ä»…è§£æï¼Œä¸ä½œåœæ­¢ç¬¦
DEFAULT_QWEN_PATHS = [
    "/root/autodl-fs/wzq/models/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218",
    "Qwen/Qwen3-8B",
]


def _is_local(path: str) -> bool:
    return os.path.exists(path) or path.startswith("/")


def build_logger(out_dir: str, local_rank: int = -1) -> logging.Logger:
    os.makedirs(out_dir, exist_ok=True)
    logger = logging.getLogger("train")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    fmt = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
    if is_main_process(local_rank):
        fh = logging.FileHandler(os.path.join(out_dir, "train.log"), encoding="utf-8")
        fh.setFormatter(fmt)
        ch = logging.StreamHandler()
        ch.setFormatter(fmt)
        logger.addHandler(fh); logger.addHandler(ch)
    else:
        logger.addHandler(logging.NullHandler())
    return logger


def check_disk_space(path: str, min_gb: float = 10.0) -> bool:
    try:
        stat = shutil.disk_usage(path)
        free_gb = stat.free / (1024**3)
        return free_gb >= min_gb
    except Exception:
        return True  # æ£€æŸ¥å¤±è´¥åˆ™å‡è®¾å……è¶³


def get_gpu_memory_info() -> str:
    if not torch.cuda.is_available():
        return "æ—  GPU"
    try:
        info_lines = []
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            total_gb = props.total_memory / (1024**3)
            info_lines.append(f"GPU{i}: {props.name} ({total_gb:.1f}GB)")
        return ", ".join(info_lines)
    except Exception as e:
        return f"æ— æ³•è·å– GPU ä¿¡æ¯: {e}"


def persist_run_config(args_namespace, out_dir: str):
    run_cfg = vars(args_namespace).copy()
    try:
        import transformers as _t; run_cfg["transformers"] = _t.__version__
    except Exception: pass
    try:
        import peft as _peft; run_cfg["peft"] = _peft.__version__
    except Exception: pass
    run_cfg["torch"] = torch.__version__
    run_cfg["cuda"] = torch.version.cuda if torch.cuda.is_available() else None
    with open(os.path.join(out_dir, "run_config.json"), "w", encoding="utf-8") as f:
        json.dump(run_cfg, f, ensure_ascii=False, indent=2)


# ---------- æ–‡æœ¬ä¸æ ¡éªŒ ----------

def get_answer(text: str) -> str:
    m = ANSWER_RE.search(text or "")
    return m.group(1).strip() if m else ""


def normalize_answer(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip()).lower()


def validate_equal(pred: str, gold: str) -> bool:
    return normalize_answer(pred) == normalize_answer(gold)


def apply_stop(decoded: str, stops: List[str]) -> str:
    earliest = len(decoded)
    for s in stops or []:
        pos = decoded.find(s)
        if pos != -1:
            earliest = min(earliest, pos)
    return decoded[:earliest] if earliest != len(decoded) else decoded


def _require_columns(ds: Dataset, cols: List[str], name: str):
    miss = [c for c in cols if c not in ds.column_names]
    if miss:
        raise ValueError(f"{name} ç¼ºå°‘å­—æ®µ: {miss}ï¼Œç°æœ‰åˆ—: {ds.column_names}")


# =========================
# å‚æ•°æ ¡éªŒ
# =========================

def validate_args(args, logger: logging.Logger) -> bool:
    issues = []
    warnings = []
    if args.lora_r <= 0:
        issues.append(f"lora_r å¿…é¡» > 0ï¼Œå½“å‰: {args.lora_r}")
    if args.lora_alpha <= 0:
        issues.append(f"lora_alpha å¿…é¡» > 0ï¼Œå½“å‰: {args.lora_alpha}")
    if args.lr <= 0:
        issues.append(f"lr å¿…é¡» > 0ï¼Œå½“å‰: {args.lr}")
    if args.epochs <= 0:
        issues.append(f"epochs å¿…é¡» > 0ï¼Œå½“å‰: {args.epochs}")
    if args.per_device_train_batch_size <= 0:
        issues.append(f"per_device_train_batch_size å¿…é¡» > 0ï¼Œå½“å‰: {args.per_device_train_batch_size}")
    if args.grad_accum <= 0:
        issues.append(f"grad_accum å¿…é¡» > 0ï¼Œå½“å‰: {args.grad_accum}")

    if args.lora_r > 256:
        warnings.append(f"lora_r è¿‡å¤§ ({args.lora_r})ï¼Œå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆæˆ–æ˜¾å­˜ä¸è¶³")
    if args.per_device_train_batch_size > 8:
        warnings.append(f"batch_size è¾ƒå¤§ ({args.per_device_train_batch_size})ï¼Œæ³¨æ„æ˜¾å­˜å ç”¨")
    if args.cutoff_len > 8192:
        warnings.append(f"cutoff_len å¾ˆå¤§ ({args.cutoff_len})ï¼Œå¯èƒ½å¯¼è‡´æ˜¾å­˜ä¸è¶³")
    if args.lr > 1e-3:
        warnings.append(f"å­¦ä¹ ç‡è¾ƒé«˜ ({args.lr})ï¼Œå¯èƒ½ä¸ç¨³å®š")
    if args.warmup_ratio > 0.5:
        warnings.append(f"warmup_ratio è¿‡å¤§ ({args.warmup_ratio})ï¼Œå¯èƒ½å½±å“è®­ç»ƒ")
    if args.load_in_4bit and args.load_in_8bit:
        issues.append("ä¸èƒ½åŒæ—¶å¯ç”¨ 4bit å’Œ 8bit é‡åŒ–")

    if issues:
        for issue in issues:
            logger.error(f"å‚æ•°é”™è¯¯: {issue}")
        raise ValueError(f"å‘ç° {len(issues)} ä¸ªå‚æ•°é”™è¯¯")

    if warnings:
        logger.warning("å‚æ•°è­¦å‘Š:")
        for warning in warnings:
            logger.warning(f"  - {warning}")
    return True


# =========================
# æ•°æ®æ ¼å¼åŒ–å‡½æ•°
# =========================

def make_messages(ex: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "messages": [
            {"role": "system", "content": ex["system"]},
            {"role": "user", "content": ex["user"]},
            {"role": "assistant", "content": ex.get("assistant", "")},
        ]
    }


def keep_eval_fields(ex: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "id": ex.get("id", None),
        "system": ex["system"],
        "user": ex["user"],
        "assistant": ex.get("assistant", ""),
        "answer": ex.get("answer", None),
    }


def create_formatting_func(tokenizer, enable_thinking_default: bool = True):
    """messages -> textï¼ˆå°½é‡å¯ç”¨æ€è€ƒæ¨¡æ¿ï¼›ä¸è¿”å› Noneï¼‰"""
    def formatting_func(example):
        messages = example["messages"]
        try:
            result = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False,
                enable_thinking=enable_thinking_default
            )
        except TypeError:
            result = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
        except Exception:
            # å›é€€åˆ°ç®€æ˜“æ¨¡æ¿
            result = ""
            for msg in messages:
                if msg["role"] == "system":
                    result += f"<|im_start|>system\n{msg['content']}<|im_end|>\n"
                elif msg["role"] == "user":
                    result += f"<|im_start|>user\n{msg['content']}<|im_end|>\n"
                elif msg["role"] == "assistant":
                    result += f"<|im_start|>assistant\n{msg['content']}<|im_end|>\n"
        if not isinstance(result, str):
            result = str(result)
        return result
    return formatting_func


# =========================
# è®­ç»ƒè¿‡ç¨‹å›è°ƒ
# =========================

class TrainingMetricsCallback(TrainerCallback):
    def __init__(self, out_dir: str, logger: Optional[logging.Logger], local_rank: int):
        self.out_dir = out_dir
        self.logger = logger or logging.getLogger(__name__)
        self.local_rank = local_rank
        os.makedirs(self.out_dir, exist_ok=True)
        self.detail_path = os.path.join(self.out_dir, "training_metrics.jsonl")
        self.summary_path = os.path.join(self.out_dir, "training_summary.csv")

    def on_train_begin(self, args, state, control, **kwargs):
        if is_main_process(self.local_rank) and not os.path.exists(self.summary_path):
            with open(self.summary_path, "w", newline="", encoding="utf-8") as f:
                csv.writer(f).writerow(["global_step","epoch","loss","learning_rate","grad_norm","timestamp"])

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is None or not is_main_process(self.local_rank):
            return
        entry = {"global_step": state.global_step, "epoch": state.epoch, **logs, "timestamp": time.time()}
        with open(self.detail_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        if "loss" in logs:
            with open(self.summary_path, "a", newline="", encoding="utf-8") as f:
                csv.writer(f).writerow([
                    state.global_step,
                    f"{state.epoch:.2f}" if state.epoch is not None else "",
                    f"{logs.get('loss',''):.6f}" if 'loss' in logs else "",
                    f"{logs.get('learning_rate',''):.2e}" if 'learning_rate' in logs else "",
                    f"{logs.get('grad_norm',''):.4f}" if 'grad_norm' in logs else "",
                    time.strftime("%Y-%m-%d %H:%M:%S"),
                ])
            if torch.cuda.is_available():
                torch.cuda.empty_cache()


class BestCheckpointCallback(TrainerCallback):
    def __init__(self, output_dir: str, save_total_limit: int, metric_name: str, logger: logging.Logger, local_rank: int):
        self.output_dir = output_dir
        self.save_total_limit = save_total_limit
        self.metric_name = metric_name
        self.logger = logger
        self.local_rank = local_rank
        self.checkpoint_info_path = os.path.join(output_dir, "checkpoint_info.json")
        self.checkpoints: List[Tuple[int, float, str]] = []
        self.best_checkpoint: Optional[Tuple[int, float, str]] = None

    def _save_state(self):
        if not is_main_process(self.local_rank): return
        data = {"checkpoints": self.checkpoints, "best_checkpoint": self.best_checkpoint}
        with open(self.checkpoint_info_path, "w") as f:
            json.dump(data, f, indent=2)

    def _cleanup(self):
        if len(self.checkpoints) <= self.save_total_limit:
            return
        self.checkpoints.sort(key=lambda x: x[0])
        keep_paths = set()
        if self.best_checkpoint:
            keep_paths.add(self.best_checkpoint[2])
        recent_limit = max(0, self.save_total_limit - 1)
        for _, _, p in self.checkpoints[-recent_limit:]:
            keep_paths.add(p)
        new_ckpts = []
        for step, acc, path in self.checkpoints:
            if path in keep_paths:
                new_ckpts.append((step, acc, path))
            else:
                try:
                    if os.path.exists(path):
                        shutil.rmtree(path)
                        self.logger.info(f"åˆ é™¤æ—§ checkpoint: {path}")
                except Exception as e:
                    self.logger.warning(f"åˆ é™¤ checkpoint å¤±è´¥ {path}: {e}")
                    new_ckpts.append((step, acc, path))
        self.checkpoints = new_ckpts

    def update(self, step: int, metric_value: float):
        if not is_main_process(self.local_rank): return
        path = os.path.join(self.output_dir, f"checkpoint-{step}")
        if not os.path.exists(path):
            self.logger.warning(f"Checkpoint ä¸å­˜åœ¨: {path}")
            return
        self.checkpoints.append((step, metric_value, path))
        if self.best_checkpoint is None or metric_value > self.best_checkpoint[1]:
            self.best_checkpoint = (step, metric_value, path)
            self.logger.info(f"ğŸ† æ–°æœ€ä½³ checkpoint: step={step}, {self.metric_name}={metric_value:.4f}")
        self._cleanup()
        self._save_state()


# =========================
# è¯„ä¼°ï¼ˆä½œä¸ºå›è°ƒè§¦å‘ï¼‰
# =========================

class EvalCallback(TrainerCallback):
    def __init__(
        self,
        tokenizer,
        val_examples: List[Dict[str, Any]],
        out_dir: str,
        logger: logging.Logger,
        checkpoint_cb: Optional[BestCheckpointCallback],
        local_rank: int,
        max_eval_samples: int = 256,
        gen_kwargs: Optional[Dict[str, Any]] = None,
        stop_tokens: Optional[List[str]] = None,
        shuffle_eval_each_time: bool = True,
        eval_batch_size: int = 1,
    ):
        self.tok = tokenizer
        self.val_examples = val_examples
        self.out_dir = out_dir
        self.logger = logger
        self.checkpoint_cb = checkpoint_cb
        self.local_rank = local_rank
        self.max_eval_samples = max_eval_samples
        self.eval_batch_size = eval_batch_size
        self.gen_kwargs = gen_kwargs or {
            "max_new_tokens": 768,
            "do_sample": True,
            "temperature": 0.3,
            "top_p": 0.9,
            "repetition_penalty": 1.05,
        }
        self.stop_tokens = stop_tokens or ["<|im_end|>"]  # ä¸å« </answer>
        os.makedirs(self.out_dir, exist_ok=True)
        self.detail_path = os.path.join(self.out_dir, "eval_detail.jsonl")
        self.summary_path = os.path.join(self.out_dir, "eval_summary.csv")
        self.shuffle_eval_each_time = shuffle_eval_each_time

    def _build_prompt(self, ex: Dict[str, Any]) -> str:
        messages = [
            {"role": "system", "content": ex["system"]},
            {"role": "user", "content": ex["user"]},
        ]
        try:
            return self.tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=True)
        except TypeError:
            return self.tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    def _gold(self, ex: Dict[str, Any]) -> str:
        if "answer" in ex and ex["answer"]:
            return str(ex["answer"]).strip()
        return get_answer(ex.get("assistant", ""))

    def on_step_end(self, args, state, control, **kwargs):
        # å…¼å®¹ Trainer çš„ save_steps/eval_stepsï¼šæ­¤å¤„ä¸ save_steps å¯¹é½è§¦å‘
        interval = getattr(args, "eval_steps", None) or args.save_steps
        if state.global_step > 0 and interval and state.global_step % interval == 0:
            model = kwargs["model"]
            self.run_eval(model, state.global_step)

    def run_eval(self, model, global_step: int):
        # ä»…ä¸»è¿›ç¨‹è¯„ä¼°ï¼Œé¿å…å¤šè¿›ç¨‹é‡å¤å†™æ–‡ä»¶
        if not is_main_process(self.local_rank):
            return
        self.logger.info(f"å¼€å§‹è¯„ä¼° (step {global_step}, batch_size={self.eval_batch_size})...")
        model.eval()

        # unwrapï¼ˆFSDP/DeepSpeed ç¯å¢ƒï¼‰
        try:
            from transformers.trainer_utils import unwrap_model
            core_model = unwrap_model(model)
        except Exception:
            core_model = getattr(model, "module", model)

        device = next(core_model.parameters()).device

        # decoder-only æ‰¹é‡ç”Ÿæˆæ—¶ä¸´æ—¶å·¦å¡«å……
        original_padding_side = self.tok.padding_side
        self.tok.padding_side = 'left'

        try:
            n_total = len(self.val_examples)
            n = min(self.max_eval_samples, n_total)
            indices = list(range(n_total))
            if self.shuffle_eval_each_time:
                random.shuffle(indices)
            indices = indices[:n]

            stats = {"n": 0, "parsed": 0, "correct": 0}
            t0 = time.time()

            # é¦–æ¬¡åˆ›å»º summary è¡¨å¤´
            if not os.path.exists(self.summary_path):
                with open(self.summary_path, "w", newline="", encoding="utf-8") as f:
                    csv.writer(f).writerow(["global_step","n","parsed","correct","accuracy","coverage","time_sec"])

            with open(self.detail_path, "a", encoding="utf-8") as fout, torch.inference_mode():
                pbar = tqdm(total=len(indices), desc=f"Eval@{global_step}", unit="sample",
                            ncols=100, disable=not is_main_process(self.local_rank))
                for batch_start in range(0, len(indices), self.eval_batch_size):
                    batch_end = min(batch_start + self.eval_batch_size, len(indices))
                    batch_indices = indices[batch_start:batch_end]
                    batch_examples = [self.val_examples[idx] for idx in batch_indices]
                    batch_prompts = [self._build_prompt(ex) for ex in batch_examples]
                    batch_golds = [self._gold(ex) for ex in batch_examples]

                    try:
                        eval_max_len = min(getattr(self.tok, "model_max_length", 4096), 4096)
                        inputs = self.tok(
                            batch_prompts,
                            return_tensors="pt",
                            padding=True,
                            truncation=True,
                            max_length=eval_max_len,
                            return_attention_mask=True
                        ).to(device)

                        outputs = core_model.generate(
                            input_ids=inputs["input_ids"],
                            attention_mask=inputs["attention_mask"],
                            pad_token_id=self.tok.pad_token_id,
                            use_cache=True,
                            **self.gen_kwargs
                        )

                        for j, (idx, ex, gold) in enumerate(zip(batch_indices, batch_examples, batch_golds)):
                            input_len = inputs["input_ids"].shape[1]
                            gen_ids = outputs[j][input_len:]
                            decoded = self.tok.decode(gen_ids, skip_special_tokens=True)
                            decoded = apply_stop(decoded, self.stop_tokens)

                            pred = get_answer(decoded)
                            parsed = pred != ""
                            stats["n"] += 1
                            if parsed:
                                stats["parsed"] += 1
                                if validate_equal(pred, gold):
                                    stats["correct"] += 1

                            fout.write(json.dumps({
                                "id": ex.get("id", idx),
                                "prompt_preview": (batch_prompts[j][:200] + "...") if len(batch_prompts[j]) > 200 else batch_prompts[j],
                                "raw_output": decoded,
                                "pred_answer": pred,
                                "gold_answer": gold,
                                "parsed": parsed,
                                "correct": parsed and validate_equal(pred, gold),
                                "global_step": global_step,
                            }, ensure_ascii=False) + "\n")

                        pbar.update(len(batch_indices))
                        pbar.set_postfix({"acc": f"{stats['correct']/max(1,stats['n']):.3f}",
                                          "cov": f"{stats['parsed']/max(1,stats['n']):.3f}"})
                    except Exception as e:
                        err_msg = str(e)
                        self.logger.warning(f"è¯„ä¼°æ‰¹æ¬¡ {batch_start}-{batch_end} å¤±è´¥: {err_msg}")
                        if "out of memory" in err_msg.lower():
                            self.logger.warning("å»ºè®®: å‡å° --eval_batch_size æˆ– --cutoff_len")
                        # é€ä¸ªå›é€€
                        for idx in batch_indices:
                            try:
                                ex = self.val_examples[idx]
                                prompt = self._build_prompt(ex)
                                gold = self._gold(ex)
                                eval_max_len = min(getattr(self.tok, "model_max_length", 4096), 4096)
                                single = self.tok(prompt, return_tensors="pt", truncation=True,
                                                  max_length=eval_max_len,
                                                  return_attention_mask=True).to(device)
                                outputs = core_model.generate(
                                    **single,
                                    pad_token_id=self.tok.pad_token_id,
                                    use_cache=True,
                                    **self.gen_kwargs
                                )
                                input_len = single["input_ids"].shape[1]
                                gen_ids = outputs[0][input_len:]
                                decoded = self.tok.decode(gen_ids, skip_special_tokens=True)
                                decoded = apply_stop(decoded, self.stop_tokens)

                                pred = get_answer(decoded)
                                parsed = pred != ""
                                stats["n"] += 1
                                if parsed:
                                    stats["parsed"] += 1
                                    if validate_equal(pred, gold):
                                        stats["correct"] += 1

                                fout.write(json.dumps({
                                    "id": ex.get("id", idx),
                                    "prompt_preview": (prompt[:200] + "...") if len(prompt) > 200 else prompt,
                                    "raw_output": decoded,
                                    "pred_answer": pred,
                                    "gold_answer": gold,
                                    "parsed": parsed,
                                    "correct": parsed and validate_equal(pred, gold),
                                    "global_step": global_step,
                                }, ensure_ascii=False) + "\n")
                            except Exception as e2:
                                self.logger.warning(f"  å•æ ·æœ¬ {idx} å¤±è´¥: {e2}")
                                continue
                        pbar.update(len(batch_indices))

                    if device.type == "cuda":
                        torch.cuda.empty_cache()

                pbar.close()

            acc = stats["correct"] / max(1, stats["n"])
            cov = stats["parsed"] / max(1, stats["n"])
            elapsed = time.time() - t0
            with open(self.summary_path, "a", newline="", encoding="utf-8") as f:
                csv.writer(f).writerow([global_step, stats["n"], stats["parsed"], stats["correct"], f"{acc:.4f}", f"{cov:.4f}", f"{elapsed:.1f}"])
            self.logger.info(f"[Eval step {global_step}] acc={acc:.4f}, coverage={cov:.4f}, n={stats['n']}, time={elapsed:.1f}s")

            if self.checkpoint_cb and global_step > 0:
                self.checkpoint_cb.update(global_step, acc)

        finally:
            self.tok.padding_side = original_padding_side
            if device.type == "cuda":
                torch.cuda.empty_cache()

        model.train()
        return acc


# =========================
# æ¨¡å‹ / Tokenizer / LoRA
# =========================

def try_enable_sdp_and_compile(model, logger):
    try:
        torch.backends.cuda.enable_flash_sdp(True)
        torch.backends.cuda.enable_mem_efficient_sdp(True)
        torch.backends.cuda.enable_math_sdp(False)
        logger.info("âœ“ å·²å¯ç”¨ SDPAï¼ˆFlash/MemEfficientï¼‰")
    except Exception as e:
        logger.warning(f"SDPA å¯ç”¨å¤±è´¥ï¼š{e}")
    try:
        model = torch.compile(model, mode="max-autotune")
        logger.info("âœ“ å·²å¯ç”¨ torch.compile(max-autotune)")
    except Exception as e:
        logger.warning(f"torch.compile å¯ç”¨å¤±è´¥ï¼š{e}")
    return model


def detect_lora_target_modules(model) -> List[str]:
    names = set()
    for name, module in model.named_modules():
        if hasattr(module, "in_features") and hasattr(module, "out_features"):
            leaf = name.split(".")[-1]
            if any(k in name for k in ("q_proj","k_proj","v_proj","o_proj","up_proj","down_proj","gate_proj")):
                names.add(leaf)
    fixed = {"q_proj","k_proj","v_proj","o_proj","up_proj","down_proj","gate_proj"}
    return sorted(set(names) | fixed)


def load_tokenizer(args, logger) -> Any:
    logger.info("åŠ è½½ Tokenizer...")
    is_local_model = _is_local(args.model_path)
    tok = AutoTokenizer.from_pretrained(
        args.model_path,
        use_fast=True,
        trust_remote_code=True,
        local_files_only=is_local_model
    )
    tok.truncation_side = "right"
    tok.padding_side = "right"
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
        logger.info(f"è®¾ç½® pad_token = eos_token ({tok.eos_token})")

    if not getattr(tok, "chat_template", None):
        logger.warning(f"æ¨¡å‹ {args.model_path} ç¼ºå°‘ chat_templateï¼Œå°è¯•ä» Qwen3-8B å¤åˆ¶...")
        copied = False
        for cand in DEFAULT_QWEN_PATHS:
            try:
                qtok = AutoTokenizer.from_pretrained(
                    cand, use_fast=True, trust_remote_code=True,
                    local_files_only=_is_local(cand)
                )
                if getattr(qtok, "chat_template", None):
                    tok.chat_template = qtok.chat_template
                    logger.info(f"âœ“ å·²ä» {cand} å¤åˆ¶ chat_template")
                    copied = True
                    break
            except Exception as e:
                logger.warning(f"ä» {cand} å¤åˆ¶å¤±è´¥: {e}")
        if not copied:
            raise ValueError("æ— æ³•è·å– chat_templateï¼›è¯·ç¡®ä¿æœ¬åœ°æˆ–ç½‘ç»œå¯ç”¨çš„ Qwen3-8B å­˜åœ¨ã€‚")

    supports_thinking = False
    try:
        _ = tok.apply_chat_template([{"role": "user", "content": "test"}],
                                    tokenize=False, add_generation_prompt=True, enable_thinking=True)
        supports_thinking = True
        logger.info("âœ“ Chat template æ”¯æŒ enable_thinking")
    except TypeError:
        logger.info("âš ï¸ Chat template ä¸æ”¯æŒ enable_thinking")

    return tok, supports_thinking


def load_model_with_lora(args, tok, logger):
    logger.info("åŠ è½½æ¨¡å‹...")
    is_local_model = _is_local(args.model_path)
    quant_config = None
    if args.load_in_4bit or args.load_in_8bit:
        try:
            from transformers import BitsAndBytesConfig
            quant_config = BitsAndBytesConfig(
                load_in_4bit=args.load_in_4bit,
                load_in_8bit=args.load_in_8bit,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True
            )
            logger.info(f"âœ“ å¯ç”¨é‡åŒ–: {'4bit' if args.load_in_4bit else '8bit'}")
        except ImportError:
            logger.error("é‡åŒ–éœ€è¦å®‰è£… bitsandbytes åº“: pip install bitsandbytes")
            raise

    try:
        model = AutoModelForCausalLM.from_pretrained(
            args.model_path,
            torch_dtype=torch.bfloat16 if args.bf16 else "auto",
            trust_remote_code=True,
            quantization_config=quant_config,
            local_files_only=is_local_model
        )
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        if "out of memory" in str(e).lower() or "oom" in str(e).lower():
            logger.error("å»ºè®®: 1) å¯ç”¨é‡åŒ– --load_in_4bit, 2) ä½¿ç”¨ DeepSpeed/FSDP, 3) å‡å°‘ batch size")
        raise

    if quant_config is not None:
        model = prepare_model_for_kbit_training(model)

    if model.config.vocab_size != len(tok):
        logger.warning(f"vocab ä¸åŒ¹é…ï¼šmodel={model.config.vocab_size}, tok={len(tok)}ï¼Œè°ƒæ•´ embedding")
        model.resize_token_embeddings(len(tok))
    model.config.use_cache = False
    if model.config.pad_token_id is None:
        model.config.pad_token_id = tok.pad_token_id

    logger.info("æ³¨å…¥ LoRA...")
    target_modules = detect_lora_target_modules(model)
    logger.info(f"  æ£€æµ‹åˆ°çš„ LoRA ç›®æ ‡å±‚: {target_modules if target_modules else 'None'}")
    if not target_modules:
        target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
        logger.info(f"  ä½¿ç”¨é»˜è®¤ç›®æ ‡å±‚: {target_modules}")

    lora_config = LoraConfig(
        r=args.lora_r, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout,
        target_modules=target_modules, task_type="CAUSAL_LM",
    )
    try:
        model = get_peft_model(model, lora_config)
    except Exception as e:
        logger.error(f"LoRA æ³¨å…¥å¤±è´¥: {e}")
        logger.error(f"å°è¯•çš„ç›®æ ‡æ¨¡å—: {target_modules}")
        raise

    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    logger.info(f"âœ“ LoRA æ³¨å…¥å®Œæˆ | å¯è®­ç»ƒå‚æ•°: {trainable:,}/{total:,} ({100*trainable/total:.2f}%)")

    model.enable_input_require_grads()
    logger.info("âœ“ å·²å¯ç”¨è¾“å…¥æ¢¯åº¦ï¼ˆgradient checkpointing æ”¯æŒï¼‰")

    if args.enable_compile:
        model = try_enable_sdp_and_compile(model, logger)

    return model


# =========================
# æ•°æ®å‡†å¤‡ï¼ˆè®­ç»ƒé›† text â†’ tokenizedï¼‰
# =========================

def prepare_datasets(args, tok, supports_thinking: bool, logger):
    logger.info("åŠ è½½æ•°æ®é›†...")
    train_raw = load_from_disk(args.train_dir)
    val_raw = load_from_disk(args.val_dir)
    logger.info(f"åŸå§‹æ•°æ®: è®­ç»ƒ={len(train_raw)}, éªŒè¯={len(val_raw)}")

    _require_columns(train_raw, ["system", "user", "assistant"], "train_raw")
    _require_columns(val_raw, ["system", "user"], "val_raw")

    def _ok_train_row(ex: Dict[str, Any]) -> bool:
        return isinstance(ex.get("system"), str) and isinstance(ex.get("user"), str) and isinstance(ex.get("assistant", ""), str)

    def _ok_eval_row(ex: Dict[str, Any]) -> bool:
        return isinstance(ex.get("system"), str) and isinstance(ex.get("user"), str)

    train_raw = train_raw.filter(_ok_train_row, num_proc=args.num_proc)
    val_raw = val_raw.filter(_ok_eval_row, num_proc=args.num_proc)
    logger.info(f"æ¸…æ´—å: è®­ç»ƒ={len(train_raw)}, éªŒè¯={len(val_raw)}")

    if args.filter_quality and args.filter_quality.strip():
        if "quality" not in train_raw.column_names:
            logger.warning(f"âš ï¸ æ•°æ®é›†ç¼ºå°‘ 'quality' å­—æ®µï¼Œè·³è¿‡è¿‡æ»¤")
        else:
            logger.info(f"è¿‡æ»¤è®­ç»ƒé›† (quality == '{args.filter_quality}')...")
            train_raw = train_raw.filter(lambda ex: ex.get("quality") == args.filter_quality, num_proc=args.num_proc)
            if len(train_raw) == 0:
                raise ValueError(f"è¿‡æ»¤åè®­ç»ƒé›†ä¸ºç©ºï¼quality='{args.filter_quality}' ä¸å­˜åœ¨ã€‚")
    else:
        logger.info("è·³è¿‡ quality è¿‡æ»¤ï¼ˆä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®ï¼‰")

    # è®­ç»ƒï¼šmessages -> textï¼ˆä»…ä¿ç•™ text å­—æ®µï¼‰
    train_ds = train_raw.map(make_messages, remove_columns=train_raw.column_names, num_proc=args.num_proc)
    enable_thinking_flag = args.enable_thinking and supports_thinking
    formatting = create_formatting_func(tok, enable_thinking_default=enable_thinking_flag)

    def _to_text(example: Dict[str, Any]) -> Dict[str, str]:
        return {"text": formatting(example)}

    train_text = train_ds.map(_to_text, remove_columns=train_ds.column_names, num_proc=args.num_proc)
    if args.max_train_samples and args.max_train_samples < len(train_text):
        train_text = train_text.select(range(args.max_train_samples))

    # è®­ç»ƒï¼šé¢„åˆ†è¯ï¼ˆåŠ¨æ€ padding æ›´é«˜æ•ˆï¼Œæ­¤å¤„ä¸åš max_length å¡«å……ï¼‰
    max_len = min(args.cutoff_len, getattr(tok, "model_max_length", args.cutoff_len))

    def tokenize_function(ex):
        out = tok(
            ex["text"],
            truncation=True,
            max_length=max_len,
            add_special_tokens=True,
        )
        # labels = input_idsï¼ˆæ ‡å‡† Causal LMï¼‰
        out["labels"] = out["input_ids"].copy()
        return out

    train_tokenized = train_text.map(
        tokenize_function,
        batched=False,
        remove_columns=train_text.column_names,
        num_proc=args.num_proc
    )

    # éªŒè¯ï¼šä¿ç•™ç»“æ„åŒ–å­—æ®µä¾›è¯„ä¼°æ„é€  prompt
    val_ds = val_raw.map(
        keep_eval_fields,
        remove_columns=[c for c in val_raw.column_names if c not in ("id", "system", "user", "assistant", "answer")],
        num_proc=args.num_proc
    )
    if len(val_ds) == 0:
        raise ValueError("éªŒè¯é›†ä¸ºç©ºæˆ–ç¼ºå°‘å¿…è¦å­—æ®µï¼")

    logger.info(f"âœ“ æ•°æ®å‡†å¤‡å®Œæˆ | è®­ç»ƒ(tokenized)={len(train_tokenized)}, éªŒè¯={len(val_ds)}")
    return train_tokenized, val_ds, enable_thinking_flag


# =========================
# ä¸»æµç¨‹
# =========================

def main():
    parser = argparse.ArgumentParser(description="BiomniGEM SFT è®­ç»ƒï¼ˆå¹¶è¡Œç¨³å¥ç‰ˆï¼‰")
    # åŸºç¡€
    parser.add_argument("--model_path", type=str, required=True)
    parser.add_argument("--train_dir", type=str, required=True)
    parser.add_argument("--val_dir", type=str, required=True)
    parser.add_argument("--output_dir", type=str, required=True)
    # LoRA
    parser.add_argument("--lora_r", type=int, default=96)
    parser.add_argument("--lora_alpha", type=int, default=16)
    parser.add_argument("--lora_dropout", type=float, default=0.05)
    # ä¼˜åŒ–
    parser.add_argument("--lr", type=float, default=2e-4)
    parser.add_argument("--weight_decay", type=float, default=0.05)
    parser.add_argument("--epochs", type=int, default=1)
    parser.add_argument("--per_device_train_batch_size", type=int, default=2)
    parser.add_argument("--grad_accum", type=int, default=32)
    parser.add_argument("--warmup_ratio", type=float, default=0.05)
    # è®­ç»ƒæ§åˆ¶
    parser.add_argument("--cutoff_len", type=int, default=4096)
    parser.add_argument("--bf16", action="store_true")
    parser.add_argument("--deepspeed", type=str, default=None)
    parser.add_argument("--resume_from_checkpoint", type=str, default=None)
    # æ—¥å¿—/ä¿å­˜/è¯„ä¼°
    parser.add_argument("--save_steps", type=int, default=400)
    parser.add_argument("--logging_steps", type=int, default=20)
    parser.add_argument("--save_total_limit", type=int, default=3)
    parser.add_argument("--initial_eval", action="store_true")
    parser.add_argument("--max_train_samples", type=int, default=None)
    parser.add_argument("--max_eval_samples_arg", type=int, default=256)
    # æ•°æ®
    parser.add_argument("--num_proc", type=int, default=4)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--packing", action="store_true")  # æ¥å£ä¿ç•™ï¼ˆæœ¬å®ç°ä¸å¯ç”¨ packingï¼Œé¿å…è·¨æ ·æœ¬æ‹¼æ¥å¸¦æ¥çš„è¾¹ç•Œæ•ˆåº”ï¼‰
    parser.add_argument("--eval_batch_size", type=int, default=None)
    parser.add_argument("--filter_quality", type=str, default="gold", help="è¿‡æ»¤è®­ç»ƒæ•°æ®çš„ quality å€¼ï¼Œè®¾ä¸ºç©ºå­—ç¬¦ä¸²åˆ™ä¸è¿‡æ»¤")
    # ç”Ÿæˆ/åœæ­¢ç¬¦ï¼ˆä¸å« </answer>ï¼‰
    parser.add_argument("--extra_stop_tokens", type=str, nargs="*", default=None)
    # æ€è€ƒæ¨¡æ¿
    parser.add_argument("--enable_thinking", action="store_true")
    # é‡åŒ–åŠ è½½ï¼ˆå¯é€‰ï¼‰
    parser.add_argument("--load_in_4bit", action="store_true")
    parser.add_argument("--load_in_8bit", action="store_true")
    # ç¼–è¯‘/SDPA
    parser.add_argument("--enable_compile", action="store_true")

    args = parser.parse_args()

    # è¿›ç¨‹ä¿¡æ¯
    local_rank = int(os.environ.get("LOCAL_RANK", -1))
    world_size = int(os.environ.get("WORLD_SIZE", 1))

    # åˆå§‹åŒ–
    os.makedirs(args.output_dir, exist_ok=True)
    set_seed(args.seed)
    logger = build_logger(args.output_dir, local_rank)

    # å‚æ•°æ ¡éªŒ
    validate_args(args, logger)

    # bf16 å¯ç”¨æ€§
    if args.bf16 and not torch.cuda.is_available():
        logger.warning("bf16 æŒ‡å®šä½†æœªæ£€æµ‹åˆ° CUDAï¼Œè‡ªåŠ¨å…³é—­ bf16")
        args.bf16 = False

    # ä¼˜é›…é€€å‡º
    def _graceful_exit(signum, frame):
        if is_main_process(local_rank):
            logger.warning(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œä¿å­˜çŠ¶æ€åé€€å‡º...")
        sys.exit(0)
    signal.signal(signal.SIGTERM, _graceful_exit)
    signal.signal(signal.SIGINT, _graceful_exit)

    logger.info("=" * 60); logger.info("BiomniGEM SFT è®­ç»ƒï¼ˆå¹¶è¡Œç¨³å¥ç‰ˆï¼‰å¯åŠ¨"); logger.info("=" * 60)
    logger.info(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    logger.info(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        logger.info(f"GPU ä¿¡æ¯: {get_gpu_memory_info()}")

    # è·¯å¾„ä¸ç£ç›˜
    is_local_model = _is_local(args.model_path)
    if is_local_model and not os.path.exists(args.model_path):
        raise FileNotFoundError(f"æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
    if not os.path.exists(args.train_dir):
        raise FileNotFoundError(f"è®­ç»ƒæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {args.train_dir}")
    if not os.path.exists(args.val_dir):
        raise FileNotFoundError(f"éªŒè¯æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {args.val_dir}")

    if not check_disk_space(args.output_dir, min_gb=10.0):
        logger.warning("âš ï¸ è¾“å‡ºç›®å½•ç£ç›˜ç©ºé—´ä¸è¶³ 10GBï¼Œè®­ç»ƒå¯èƒ½å¤±è´¥")

    # Tokenizer
    tok, supports_thinking = load_tokenizer(args, logger)
    logger.info(f"âœ“ Tokenizer å°±ç»ª | è¯è¡¨: {len(tok)} | EOS: {tok.eos_token} | padding_side(train)={tok.padding_side}")

    # æ¨¡å‹+LoRA
    model = load_model_with_lora(args, tok, logger)
    logger.info(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ | dtype: {model.dtype}")

    # æ•°æ®
    train_tokenized, val_ds, enable_thinking_flag = prepare_datasets(args, tok, supports_thinking, logger)

    # é…ç½®æ‘˜è¦
    logger.info("=" * 60)
    logger.info("è®­ç»ƒé…ç½®æ‘˜è¦:")
    logger.info(f"  æ¨¡å‹: {args.model_path}")
    logger.info(f"  è¾“å‡º: {args.output_dir}")
    logger.info(f"  LoRA: r={args.lora_r}, alpha={args.lora_alpha}, dropout={args.lora_dropout}")
    logger.info(f"  ä¼˜åŒ–: lr={args.lr}, weight_decay={args.weight_decay}, warmup={args.warmup_ratio}")
    logger.info(f"  è®­ç»ƒ: epochs={args.epochs}, batch={args.per_device_train_batch_size}, grad_accum={args.grad_accum}")
    logger.info(f"  æ•°æ®: è®­ç»ƒæ ·æœ¬={len(train_tokenized)}, éªŒè¯æ ·æœ¬={len(val_ds)}, cutoff={args.cutoff_len}")
    logger.info(f"  ç²¾åº¦: bf16={args.bf16}, 4bit={args.load_in_4bit}, 8bit={args.load_in_8bit}")
    logger.info(f"  å…¶ä»–: packing={args.packing}, thinking={args.enable_thinking}, compile={args.enable_compile}")
    logger.info(f"  å¹¶è¡Œ: world_size={world_size}, local_rank={local_rank}")
    logger.info("=" * 60)

    # DataCollatorï¼ˆåŠ¨æ€ paddingï¼Œæ›´çœæ˜¾å­˜ï¼‰
    data_collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)

    # Trainer/Accelerate è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        learning_rate=args.lr,
        weight_decay=args.weight_decay,
        lr_scheduler_type="cosine",
        warmup_ratio=args.warmup_ratio,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.grad_accum,
        gradient_checkpointing=True,
        bf16=args.bf16,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        evaluation_strategy="no",     # æˆ‘ä»¬ç”¨è‡ªå®šä¹‰ EvalCallback å®šæ—¶è¯„ä¼°
        save_total_limit=args.save_total_limit,
        logging_first_step=True,
        report_to=[],                 # å…³é—­ wandb ç­‰å¤–éƒ¨æ—¥å¿—
        dataloader_pin_memory=True,
        deepspeed=args.deepspeed,     # å¯é€‰ï¼šä¼ å…¥ deepspeed é…ç½® json è·¯å¾„
    )

    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_tokenized,
        tokenizer=tok,
        data_collator=data_collator,
    )

    # å›è°ƒï¼šè®­ç»ƒæŒ‡æ ‡ / æœ€ä¼˜ ckpt
    metrics_cb = TrainingMetricsCallback(
        out_dir=os.path.join(args.output_dir, "metrics"),
        logger=logger,
        local_rank=local_rank
    )
    trainer.add_callback(metrics_cb)

    ckpt_cb = BestCheckpointCallback(
        output_dir=args.output_dir,
        save_total_limit=args.save_total_limit,
        metric_name="eval_accuracy",
        logger=logger,
        local_rank=local_rank
    )
    trainer.add_callback(ckpt_cb)

    # è¯„ä¼°å‡†å¤‡
    val_examples = [val_ds[i] for i in range(len(val_ds))]
    eval_batch_size = args.eval_batch_size if args.eval_batch_size is not None else args.per_device_train_batch_size
    logger.info(f"è¯„ä¼°æ‰¹æ¬¡å¤§å°: {eval_batch_size}")

    # ç”Ÿæˆé…ç½®
    eos_ids: List[int] = []
    for tok_text in ("<|im_end|>", tok.eos_token):
        try:
            _id = tok.convert_tokens_to_ids(tok_text)
            if isinstance(_id, int) and _id != tok.unk_token_id:
                eos_ids.append(_id)
        except Exception:
            pass
    gen_kwargs = {
        "max_new_tokens": 768,
        "do_sample": True,
        "temperature": 0.3,
        "top_p": 0.9,
        "repetition_penalty": 1.05,
    }
    if eos_ids:
        gen_kwargs["eos_token_id"] = eos_ids if len(eos_ids) > 1 else eos_ids[0]

    stop_tokens = args.extra_stop_tokens if args.extra_stop_tokens is not None else ["<|im_end|>"]

    eval_cb = EvalCallback(
        tokenizer=tok,
        val_examples=val_examples,
        out_dir=os.path.join(args.output_dir, "eval"),
        logger=logger,
        checkpoint_cb=ckpt_cb,
        local_rank=local_rank,
        max_eval_samples=args.max_eval_samples_arg,
        gen_kwargs=gen_kwargs,
        stop_tokens=stop_tokens,
        shuffle_eval_each_time=True,
        eval_batch_size=eval_batch_size,
    )
    trainer.add_callback(eval_cb)

    # åˆå§‹è¯„ä¼°ï¼ˆå¯é€‰ï¼Œä»…ä¸»è¿›ç¨‹ï¼‰
    if args.initial_eval and is_main_process(local_rank):
        logger.info("=" * 60); logger.info("åˆå§‹è¯„ä¼°ï¼ˆè®­ç»ƒå‰ï¼‰"); logger.info("=" * 60)
        eval_cb.run_eval(trainer.model, global_step=0)
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("âœ“ å·²æ¸…ç†è¯„ä¼°åçš„æ˜¾å­˜ç¼“å­˜")

    # è½ç›˜è¿è¡Œé…ç½®ä¸ tokenizerï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    if is_main_process(local_rank):
        persist_run_config(args, args.output_dir)
        tok.save_pretrained(args.output_dir)

    # æ‰“å°è®­ç»ƒè§„æ¨¡ä¼°è®¡ï¼ˆä¿¡æ¯æ€§ï¼Œä¸å½±å“å¹¶è¡Œï¼‰
    total_train_samples = len(train_tokenized)
    per_device_batch = args.per_device_train_batch_size
    grad_accum = args.grad_accum
    effective_batch = per_device_batch * grad_accum * max(1, world_size)
    steps_per_epoch = total_train_samples // max(1, effective_batch)
    total_steps = steps_per_epoch * args.epochs
    if is_main_process(local_rank):
        logger.info(f"è®­ç»ƒè¿›åº¦ä¼°è®¡: {steps_per_epoch} steps/epoch Ã— {args.epochs} epochs = {total_steps} total steps")
        logger.info(f"æœ‰æ•ˆ batch size: {per_device_batch} Ã— {grad_accum} Ã— {max(1,world_size)} GPU = {effective_batch}")

    # è®­ç»ƒ
    logger.info("=" * 60); logger.info("å¼€å§‹è®­ç»ƒ..."); logger.info("=" * 60)
    try:
        trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)
    except RuntimeError as e:
        err_msg = str(e)
        if "CUDA out of memory" in err_msg or "out of memory" in err_msg.lower():
            if is_main_process(local_rank):
                logger.error("=" * 60)
                logger.error("æ£€æµ‹åˆ°æ˜¾å­˜ä¸è¶³ (OOM)")
                logger.error("=" * 60)
                logger.error("å»ºè®®æªæ–½ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š")
                logger.error("  1. å‡å° batch size: --per_device_train_batch_size 1")
                logger.error("  2. å¢åŠ æ¢¯åº¦ç´¯ç§¯: --grad_accum 64")
                logger.error("  3. å¯ç”¨ 4bit é‡åŒ–: --load_in_4bit")
                logger.error("  4. å‡å°åºåˆ—é•¿åº¦: --cutoff_len 2048")
                logger.error("  5. å‡å° LoRA rank: --lora_r 64")
                logger.error(f"  å½“å‰é…ç½®: batch={per_device_batch}, grad_accum={grad_accum}, cutoff={args.cutoff_len}, lora_r={args.lora_r}")
                logger.error("=" * 60)
        raise
    except KeyboardInterrupt:
        if is_main_process(local_rank):
            logger.warning("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå°è¯•ä¿å­˜çŠ¶æ€...")
            try:
                trainer.save_state()
                logger.info("âœ“ çŠ¶æ€å·²ä¿å­˜")
            except Exception as e:
                logger.warning(f"çŠ¶æ€ä¿å­˜å¤±è´¥: {e}")
        raise
    finally:
        if is_main_process(local_rank):
            try:
                trainer.save_state()
            except Exception:
                pass

    # æœ€ç»ˆè¯„ä¼°ä¸ä¿å­˜ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    if is_main_process(local_rank):
        logger.info("=" * 60); logger.info("æœ€ç»ˆè¯„ä¼°..."); logger.info("=" * 60)
        eval_cb.run_eval(trainer.model, global_step=-1)
        logger.info("=" * 60); logger.info("ä¿å­˜æ¨¡å‹ä¸ tokenizer..."); logger.info("=" * 60)
        trainer.save_model(args.output_dir)
        tok.save_pretrained(args.output_dir)
        try:
            if hasattr(trainer.model, "save_pretrained"):
                trainer.model.save_pretrained(os.path.join(args.output_dir, "adapter"))
        except Exception:
            pass
        logger.info("è®­ç»ƒå®Œæˆ âœ…")


if __name__ == "__main__":
    main()
