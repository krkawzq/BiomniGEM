#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BiomniGEM SFT è®­ç»ƒè„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
------------------------------------------------------------
æ ¸å¿ƒæ”¹è¿›ï¼š
- âœ… è¯„ä¼°æ‰¹é‡æ”¯æŒ + è¿›åº¦æ¡ï¼ˆå¯è®¾ç½® eval_batch_sizeï¼‰
- âœ… æ¨ç†æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å·¦å¡«å……ï¼ˆdecoder-only æ¨¡å‹è¦æ±‚ï¼‰
- âœ… æ­£ç¡®çš„åºåˆ—åˆ‡åˆ†ï¼šä½¿ç”¨å›ºå®šè¾“å…¥é•¿åº¦è€Œé attention_mask.sum()
- âœ… æˆªæ–­ç­–ç•¥ï¼štruncation_side='right'ï¼ˆä¿ç•™å¼€å¤´ system promptï¼‰
- âœ… æœ¬åœ°/è¿œç¨‹æ¨¡å‹è‡ªåŠ¨åˆ¤å®šï¼ˆlocal_files_onlyï¼‰
- âœ… generation_config å¯¹é½ pad/eosï¼Œå‡å°‘è­¦å‘Š
- âœ… è®­ç»ƒå‰æ•°æ®æ¸…æ´—ï¼ˆè¿‡æ»¤æ— æ•ˆæ ·æœ¬ï¼‰
- âœ… è¯„ä¼°æ‰¹æœ«æ˜¾å­˜æ¸…ç†ï¼ˆtorch.cuda.empty_cacheï¼‰
- âœ… æœ€ä¼˜ checkpoint ç®¡ç†ï¼ˆä¿ç•™æœ€ä½³ + æœ€è¿‘ k-1ï¼‰
- âœ… å»æ‰ </answer> ä½œä¸ºåœæ­¢ç¬¦ï¼Œä»…ç”¨äºç­”æ¡ˆè§£æ
"""

import os
import re
import json
import csv
import sys
import time
import shutil
import logging
import random
import signal
import argparse
from typing import Dict, Any, List, Optional, Tuple

import torch
from datasets import load_from_disk
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainerCallback,
    set_seed,
)
from transformers.trainer_utils import is_main_process
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer, SFTConfig
from tqdm import tqdm

# =========================
# å¸¸é‡ / å·¥å…·
# =========================

ANSWER_RE = re.compile(r"<answer>(.*?)</answer>", flags=re.S)  # ä»…è§£æï¼Œä¸ä½œåœæ­¢ç¬¦
DEFAULT_QWEN_PATHS = [
    "/root/autodl-fs/hf-models/Qwen--Qwen3-8B",
    "Qwen/Qwen3-8B",
]

def _is_local(path: str) -> bool:
    return os.path.exists(path) or path.startswith("/")

def build_logger(out_dir: str, local_rank: int = -1) -> logging.Logger:
    os.makedirs(out_dir, exist_ok=True)
    logger = logging.getLogger("train")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    fmt = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
    if is_main_process(local_rank):
        fh = logging.FileHandler(os.path.join(out_dir, "train.log"), encoding="utf-8")
        fh.setFormatter(fmt)
        ch = logging.StreamHandler()
        ch.setFormatter(fmt)
        logger.addHandler(fh); logger.addHandler(ch)
    else:
        logger.addHandler(logging.NullHandler())
    return logger

def get_answer(text: str) -> str:
    m = ANSWER_RE.search(text or "")
    return m.group(1).strip() if m else ""

def normalize_answer(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip()).lower()

def validate_equal(pred: str, gold: str) -> bool:
    return normalize_answer(pred) == normalize_answer(gold)

def apply_stop(decoded: str, stops: List[str]) -> str:
    """é‡åˆ°ä»»æ„ stop å­—ç¬¦ä¸²å³æˆªæ–­ï¼ˆä¸åŒ…å«åœç¬¦æœ¬èº«ï¼‰"""
    earliest = len(decoded)
    for s in stops or []:
        pos = decoded.find(s)
        if pos != -1:
            earliest = min(earliest, pos)
    return decoded[:earliest] if earliest != len(decoded) else decoded

def _require_columns(ds, cols: List[str], name: str):
    miss = [c for c in cols if c not in ds.column_names]
    if miss:
        raise ValueError(f"{name} ç¼ºå°‘å­—æ®µ: {miss}ï¼Œç°æœ‰åˆ—: {ds.column_names}")

def _ok_train_row(ex: Dict[str, Any]) -> bool:
    return isinstance(ex.get("system"), str) and isinstance(ex.get("user"), str) and isinstance(ex.get("assistant", ""), str)

def _ok_eval_row(ex: Dict[str, Any]) -> bool:
    return isinstance(ex.get("system"), str) and isinstance(ex.get("user"), str)

def detect_lora_target_modules(model) -> List[str]:
    """è‡ªåŠ¨æ¢æµ‹ + å¸¸è§å­ä¸²å¹¶é›†ï¼Œå®¹å¿å‘½åå·®å¼‚"""
    names = set()
    for name, module in model.named_modules():
        if hasattr(module, "in_features") and hasattr(module, "out_features"):
            leaf = name.split(".")[-1]
            if any(k in name for k in ("q_proj","k_proj","v_proj","o_proj","up_proj","down_proj","gate_proj")):
                names.add(leaf)
    fixed = {"q_proj","k_proj","v_proj","o_proj","up_proj","down_proj","gate_proj"}
    return sorted(set(names) | fixed)

def try_enable_sdp_and_compile(model, logger):
    try:
        torch.backends.cuda.enable_flash_sdp(True)
        torch.backends.cuda.enable_mem_efficient_sdp(True)
        torch.backends.cuda.enable_math_sdp(False)
        logger.info("âœ“ å·²å¯ç”¨ SDPAï¼ˆFlash/MemEfficientï¼‰")
    except Exception as e:
        logger.warning(f"SDPA å¯ç”¨å¤±è´¥ï¼š{e}")
    try:
        model = torch.compile(model, mode="max-autotune")
        logger.info("âœ“ å·²å¯ç”¨ torch.compile(max-autotune)")
    except Exception as e:
        logger.warning(f"torch.compile å¯ç”¨å¤±è´¥ï¼š{e}")
    return model

def persist_run_config(args_namespace, out_dir: str):
    run_cfg = vars(args_namespace).copy()
    try:
        import transformers as _t; run_cfg["transformers"] = _t.__version__
    except Exception: pass
    try:
        import trl as _trl; run_cfg["trl"] = _trl.__version__
    except Exception: pass
    try:
        import peft as _peft; run_cfg["peft"] = _peft.__version__
    except Exception: pass
    run_cfg["torch"] = torch.__version__
    run_cfg["cuda"] = torch.version.cuda if torch.cuda.is_available() else None
    with open(os.path.join(out_dir, "run_config.json"), "w", encoding="utf-8") as f:
        json.dump(run_cfg, f, ensure_ascii=False, indent=2)

# =========================
# æ•°æ®æ ¼å¼åŒ–
# =========================

def make_messages(ex: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "messages": [
            {"role": "system", "content": ex["system"]},
            {"role": "user", "content": ex["user"]},
            {"role": "assistant", "content": ex.get("assistant", "")},
        ]
    }

def keep_eval_fields(ex: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "id": ex.get("id", None),
        "system": ex["system"],
        "user": ex["user"],
        "assistant": ex.get("assistant", ""),
        "answer": ex.get("answer", None),
    }

def create_formatting_func(tokenizer, enable_thinking_default: bool = True):
    """messages -> textï¼ˆå°½é‡å¯ç”¨æ€è€ƒæ¨¡æ¿ï¼›ä¸è¿”å› Noneï¼‰"""
    def formatting_func(example):
        messages = example["messages"]
        try:
            return tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False,
                enable_thinking=enable_thinking_default
            )
        except TypeError:
            return tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
    return formatting_func

# =========================
# å›è°ƒï¼šæŒ‡æ ‡ä¸æœ€ä¼˜ ckpt
# =========================

class TrainingMetricsCallback(TrainerCallback):
    def __init__(self, out_dir: str, logger: Optional[logging.Logger], local_rank: int):
        self.out_dir = out_dir
        self.logger = logger or logging.getLogger(__name__)
        self.local_rank = local_rank
        os.makedirs(self.out_dir, exist_ok=True)
        self.detail_path = os.path.join(self.out_dir, "training_metrics.jsonl")
        self.summary_path = os.path.join(self.out_dir, "training_summary.csv")
        if is_main_process(self.local_rank) and not os.path.exists(self.summary_path):
            with open(self.summary_path, "w", newline="", encoding="utf-8") as f:
                csv.writer(f).writerow(["global_step","epoch","loss","learning_rate","grad_norm","timestamp"])

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is None or not is_main_process(self.local_rank):
            return
        entry = {"global_step": state.global_step, "epoch": state.epoch, **logs, "timestamp": time.time()}
        with open(self.detail_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        if "loss" in logs:
            with open(self.summary_path, "a", newline="", encoding="utf-8") as f:
                csv.writer(f).writerow([
                    state.global_step,
                    f"{state.epoch:.2f}" if state.epoch is not None else "",
                    f"{logs.get('loss',''):.6f}" if 'loss' in logs else "",
                    f"{logs.get('learning_rate',''):.2e}" if 'learning_rate' in logs else "",
                    f"{logs.get('grad_norm',''):.4f}" if 'grad_norm' in logs else "",
                    time.strftime("%Y-%m-%d %H:%M:%S"),
                ])

class BestCheckpointCallback(TrainerCallback):
    def __init__(self, output_dir: str, save_total_limit: int, metric_name: str, logger: logging.Logger, local_rank: int):
        self.output_dir = output_dir
        self.save_total_limit = save_total_limit
        self.metric_name = metric_name
        self.logger = logger
        self.local_rank = local_rank
        self.checkpoint_info_path = os.path.join(output_dir, "checkpoint_info.json")
        self.checkpoints: List[Tuple[int, float, str]] = []
        self.best_checkpoint: Optional[Tuple[int, float, str]] = None
        self._load()

    def _load(self):
        if os.path.exists(self.checkpoint_info_path):
            try:
                with open(self.checkpoint_info_path, "r") as f:
                    data = json.load(f)
                self.checkpoints = data.get("checkpoints", [])
                self.best_checkpoint = data.get("best_checkpoint")
                self.logger.info(f"åŠ è½½ checkpoint è®°å½•ï¼š{len(self.checkpoints)}")
            except Exception as e:
                self.logger.warning(f"åŠ è½½ checkpoint ä¿¡æ¯å¤±è´¥: {e}")

    def _save(self):
        if not is_main_process(self.local_rank): return
        data = {"checkpoints": self.checkpoints, "best_checkpoint": self.best_checkpoint}
        with open(self.checkpoint_info_path, "w") as f:
            json.dump(data, f, indent=2)

    def update(self, step: int, metric_value: float):
        if not is_main_process(self.local_rank): return
        path = os.path.join(self.output_dir, f"checkpoint-{step}")
        if not os.path.exists(path):
            self.logger.warning(f"Checkpoint ä¸å­˜åœ¨: {path}")
            return
        self.checkpoints.append((step, metric_value, path))
        if self.best_checkpoint is None or metric_value > self.best_checkpoint[1]:
            self.best_checkpoint = (step, metric_value, path)
            self.logger.info(f"ğŸ† æ–°æœ€ä½³ checkpoint: step={step}, {self.metric_name}={metric_value:.4f}")
        self._cleanup()
        self._save()

    def _cleanup(self):
        if len(self.checkpoints) <= self.save_total_limit:
            return
        self.checkpoints.sort(key=lambda x: x[0])
        keep_paths = set()
        if self.best_checkpoint:
            keep_paths.add(self.best_checkpoint[2])
        recent_limit = max(0, self.save_total_limit - 1)
        for _, _, p in self.checkpoints[-recent_limit:]:
            keep_paths.add(p)
        new_ckpts = []
        for step, acc, path in self.checkpoints:
            if path in keep_paths: new_ckpts.append((step, acc, path))
            else:
                try:
                    shutil.rmtree(path)
                    self.logger.info(f"åˆ é™¤æ—§ checkpoint: {path}")
                except Exception as e:
                    self.logger.warning(f"åˆ é™¤ checkpoint å¤±è´¥ {path}: {e}")
        self.checkpoints = new_ckpts

# =========================
# è¯„ä¼°
# =========================

class EvalCallback(TrainerCallback):
    def __init__(
        self,
        tokenizer,
        val_examples: List[Dict[str, Any]],
        out_dir: str,
        logger: logging.Logger,
        checkpoint_cb: Optional[BestCheckpointCallback],
        local_rank: int,
        max_eval_samples: int = 256,
        gen_kwargs: Optional[Dict[str, Any]] = None,
        stop_tokens: Optional[List[str]] = None,
        shuffle_eval_each_time: bool = True,
        eval_batch_size: int = 1,
    ):
        self.tok = tokenizer
        self.val_examples = val_examples
        self.out_dir = out_dir
        self.logger = logger
        self.checkpoint_cb = checkpoint_cb
        self.local_rank = local_rank
        self.max_eval_samples = max_eval_samples
        self.eval_batch_size = eval_batch_size
        self.gen_kwargs = gen_kwargs or {
            "max_new_tokens": 768,
            "do_sample": True,
            "temperature": 0.3,
            "top_p": 0.9,
            "repetition_penalty": 1.05,
        }
        self.stop_tokens = stop_tokens or ["<|im_end|>"]  # ä¸å« </answer>
        os.makedirs(self.out_dir, exist_ok=True)
        self.detail_path = os.path.join(self.out_dir, "eval_detail.jsonl")
        self.summary_path = os.path.join(self.out_dir, "eval_summary.csv")
        self.shuffle_eval_each_time = shuffle_eval_each_time
        if is_main_process(self.local_rank) and not os.path.exists(self.summary_path):
            with open(self.summary_path, "w", newline="", encoding="utf-8") as f:
                csv.writer(f).writerow(["global_step","n","parsed","correct","accuracy","coverage","time_sec"])

    def _build_prompt(self, ex: Dict[str, Any]) -> str:
        messages = [
            {"role": "system", "content": ex["system"]},
            {"role": "user", "content": ex["user"]},
        ]
        try:
            return self.tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=True)
        except TypeError:
            return self.tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    def _gold(self, ex: Dict[str, Any]) -> str:
        if "answer" in ex and ex["answer"]:
            return str(ex["answer"]).strip()
        return get_answer(ex.get("assistant", ""))

    def on_step_end(self, args, state, control, **kwargs):
        interval = getattr(args, "eval_steps", None) or args.save_steps
        if state.global_step > 0 and interval and state.global_step % interval == 0:
            self.run_eval(kwargs["model"], state.global_step)

    def run_eval(self, model, global_step: int):
        if not is_main_process(self.local_rank):
            return
        self.logger.info(f"å¼€å§‹è¯„ä¼° (step {global_step}, batch_size={self.eval_batch_size})...")
        model.eval()
        device = next(model.parameters()).device

        # ä¸´æ—¶å·¦å¡«å……ï¼ˆdecoder-only æ‰¹é‡ç”Ÿæˆéœ€è¦ï¼‰
        original_padding_side = self.tok.padding_side
        self.tok.padding_side = 'left'

        try:
            n_total = len(self.val_examples)
            n = min(self.max_eval_samples, n_total)
            indices = list(range(n_total))
            if self.shuffle_eval_each_time:
                random.shuffle(indices)
            indices = indices[:n]

            stats = {"n": 0, "parsed": 0, "correct": 0}
            t0 = time.time()

            with open(self.detail_path, "a", encoding="utf-8") as fout, torch.inference_mode():
                pbar = tqdm(total=len(indices), desc=f"Eval@{global_step}", unit="sample",
                            ncols=100, disable=not is_main_process(self.local_rank))
                for batch_start in range(0, len(indices), self.eval_batch_size):
                    batch_end = min(batch_start + self.eval_batch_size, len(indices))
                    batch_indices = indices[batch_start:batch_end]
                    batch_examples = [self.val_examples[idx] for idx in batch_indices]
                    batch_prompts = [self._build_prompt(ex) for ex in batch_examples]
                    batch_golds = [self._gold(ex) for ex in batch_examples]

                    try:
                        inputs = self.tok(
                            batch_prompts,
                            return_tensors="pt",
                            padding=True,
                            truncation=True,
                            max_length=getattr(self.tok, "model_max_length", 4096),
                            return_attention_mask=True
                        ).to(device)

                        outputs = model.generate(
                            input_ids=inputs["input_ids"],
                            attention_mask=inputs["attention_mask"],
                            pad_token_id=self.tok.pad_token_id,
                            use_cache=True,
                            **self.gen_kwargs
                        )

                        for j, (idx, ex, gold) in enumerate(zip(batch_indices, batch_examples, batch_golds)):
                            # åˆ‡åˆ†ç”Ÿæˆéƒ¨åˆ†ï¼šä»è¾“å…¥åºåˆ—é•¿åº¦åå¼€å§‹ï¼ˆåŒ…å«paddingï¼‰
                            input_len = inputs["input_ids"].shape[1]
                            gen_ids = outputs[j][input_len:]
                            decoded = self.tok.decode(gen_ids, skip_special_tokens=True)
                            decoded = apply_stop(decoded, self.stop_tokens)

                            pred = get_answer(decoded)
                            parsed = pred != ""
                            stats["n"] += 1
                            if parsed:
                                stats["parsed"] += 1
                                if validate_equal(pred, gold):
                                    stats["correct"] += 1

                            fout.write(json.dumps({
                                "id": ex.get("id", idx),
                                "prompt_preview": (batch_prompts[j][:200] + "...") if len(batch_prompts[j]) > 200 else batch_prompts[j],
                                "raw_output": decoded,
                                "pred_answer": pred,
                                "gold_answer": gold,
                                "parsed": parsed,
                                "correct": parsed and validate_equal(pred, gold),
                                "global_step": global_step,
                            }, ensure_ascii=False) + "\n")

                        pbar.update(len(batch_indices))
                        pbar.set_postfix({"acc": f"{stats['correct']/max(1,stats['n']):.3f}",
                                          "cov": f"{stats['parsed']/max(1,stats['n']):.3f}"})
                    except Exception as e:
                        self.logger.warning(f"è¯„ä¼°æ‰¹æ¬¡ {batch_start}-{batch_end} å¤±è´¥: {e}")
                        # å›é€€é€ä¸ª
                        for idx in batch_indices:
                            try:
                                ex = self.val_examples[idx]
                                prompt = self._build_prompt(ex)
                                gold = self._gold(ex)
                                single = self.tok(prompt, return_tensors="pt", truncation=True,
                                                  max_length=getattr(self.tok, "model_max_length", 4096),
                                                  return_attention_mask=True).to(device)
                                outputs = model.generate(
                                    **single,
                                    pad_token_id=self.tok.pad_token_id,
                                    use_cache=True,
                                    **self.gen_kwargs
                                )
                                input_len = single["input_ids"].shape[1]
                                gen_ids = outputs[0][input_len:]
                                decoded = self.tok.decode(gen_ids, skip_special_tokens=True)
                                decoded = apply_stop(decoded, self.stop_tokens)

                                pred = get_answer(decoded)
                                parsed = pred != ""
                                stats["n"] += 1
                                if parsed:
                                    stats["parsed"] += 1
                                    if validate_equal(pred, gold):
                                        stats["correct"] += 1

                                fout.write(json.dumps({
                                    "id": ex.get("id", idx),
                                    "prompt_preview": (prompt[:200] + "...") if len(prompt) > 200 else prompt,
                                    "raw_output": decoded,
                                    "pred_answer": pred,
                                    "gold_answer": gold,
                                    "parsed": parsed,
                                    "correct": parsed and validate_equal(pred, gold),
                                    "global_step": global_step,
                                }, ensure_ascii=False) + "\n")
                            except Exception as e2:
                                self.logger.warning(f"  å•æ ·æœ¬ {idx} å¤±è´¥: {e2}")
                                continue
                        pbar.update(len(batch_indices))

                    # æ‰¹æœ«æ¸…ç†æ˜¾å­˜ç¢ç‰‡
                    if device.type == "cuda":
                        torch.cuda.empty_cache()

                pbar.close()

            acc = stats["correct"] / max(1, stats["n"])
            cov = stats["parsed"] / max(1, stats["n"])
            elapsed = time.time() - t0
            with open(self.summary_path, "a", newline="", encoding="utf-8") as f:
                csv.writer(f).writerow([global_step, stats["n"], stats["parsed"], stats["correct"], f"{acc:.4f}", f"{cov:.4f}", f"{elapsed:.1f}"])
            self.logger.info(f"[Eval step {global_step}] acc={acc:.4f}, coverage={cov:.4f}, n={stats['n']}, time={elapsed:.1f}s")

            if self.checkpoint_cb and global_step > 0:
                self.checkpoint_cb.update(global_step, acc)

        finally:
            self.tok.padding_side = original_padding_side

        model.train()
        return acc

# =========================
# ä¸»æµç¨‹
# =========================

def main():
    parser = argparse.ArgumentParser(description="BiomniGEM SFT è®­ç»ƒï¼ˆç¨³å¥æ¸…æ™°ç‰ˆï¼‰")
    # åŸºç¡€
    parser.add_argument("--model_path", type=str, required=True)
    parser.add_argument("--train_dir", type=str, required=True)
    parser.add_argument("--val_dir", type=str, required=True)
    parser.add_argument("--output_dir", type=str, required=True)
    # LoRA
    parser.add_argument("--lora_r", type=int, default=96)
    parser.add_argument("--lora_alpha", type=int, default=16)
    parser.add_argument("--lora_dropout", type=float, default=0.05)
    # ä¼˜åŒ–
    parser.add_argument("--lr", type=float, default=2e-4)
    parser.add_argument("--weight_decay", type=float, default=0.05)
    parser.add_argument("--epochs", type=int, default=1)
    parser.add_argument("--per_device_train_batch_size", type=int, default=2)
    parser.add_argument("--grad_accum", type=int, default=32)
    parser.add_argument("--warmup_ratio", type=float, default=0.05)
    # è®­ç»ƒæ§åˆ¶
    parser.add_argument("--cutoff_len", type=int, default=4096)
    parser.add_argument("--bf16", action="store_true")
    parser.add_argument("--deepspeed", type=str, default=None)
    parser.add_argument("--resume_from_checkpoint", type=str, default=None)
    # æ—¥å¿—/ä¿å­˜/è¯„ä¼°
    parser.add_argument("--save_steps", type=int, default=400)
    parser.add_argument("--logging_steps", type=int, default=20)
    parser.add_argument("--save_total_limit", type=int, default=3)
    parser.add_argument("--initial_eval", action="store_true")
    parser.add_argument("--max_train_samples", type=int, default=None)
    parser.add_argument("--max_eval_samples_arg", type=int, default=256)
    # æ•°æ®
    parser.add_argument("--num_proc", type=int, default=4)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--packing", action="store_true")
    parser.add_argument("--eval_batch_size", type=int, default=None)
    # ç”Ÿæˆ/åœæ­¢ç¬¦ï¼ˆä¸å« </answer>ï¼‰
    parser.add_argument("--extra_stop_tokens", type=str, nargs="*", default=None)
    # æ€è€ƒæ¨¡æ¿
    parser.add_argument("--enable_thinking", action="store_true")
    # é‡åŒ–åŠ è½½ï¼ˆå¯é€‰ï¼‰
    parser.add_argument("--load_in_4bit", action="store_true")
    parser.add_argument("--load_in_8bit", action="store_true")
    # ç¼–è¯‘/SDPA
    parser.add_argument("--enable_compile", action="store_true")

    args = parser.parse_args()

    # åˆå§‹åŒ–
    os.makedirs(args.output_dir, exist_ok=True)
    local_rank = int(os.environ.get("LOCAL_RANK", "-1"))
    logger = build_logger(args.output_dir, local_rank)
    set_seed(args.seed)

    # bf16 å¯ç”¨æ€§
    if args.bf16 and not torch.cuda.is_available():
        logger.warning("bf16 æŒ‡å®šä½†æœªæ£€æµ‹åˆ° CUDAï¼Œè‡ªåŠ¨å…³é—­ bf16")
        args.bf16 = False

    # ä¼˜é›…é€€å‡º
    def _graceful_exit(signum, frame):
        if is_main_process(local_rank):
            logger.warning(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œä¿å­˜çŠ¶æ€åé€€å‡º...")
        sys.exit(0)
    signal.signal(signal.SIGTERM, _graceful_exit)
    signal.signal(signal.SIGINT,  _graceful_exit)

    logger.info("="*60); logger.info("BiomniGEM SFT è®­ç»ƒï¼ˆç¨³å¥æ¸…æ™°ç‰ˆï¼‰å¯åŠ¨"); logger.info("="*60)

    # åŠ è½½ tokenizerï¼ˆæœ¬åœ°/è¿œç¨‹è‡ªåŠ¨åˆ¤å®šï¼‰
    is_local_model = _is_local(args.model_path)
    logger.info("åŠ è½½ Tokenizer...")
    tok = AutoTokenizer.from_pretrained(
        args.model_path,
        use_fast=True,
        trust_remote_code=True,
        local_files_only=is_local_model
    )

    # æˆªæ–­ä¸å¡«å……ç­–ç•¥
    tok.truncation_side = "right"  # ä¿ç•™å¼€å¤´ï¼ˆsystem promptï¼‰
    tok.padding_side    = "right"  # è®­ç»ƒå³å¡«å……ï¼›è¯„ä¼°ä¼šä¸´æ—¶æ”¹ left
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
        logger.info(f"è®¾ç½® pad_token = eos_token ({tok.eos_token})")

    # chat_template æ£€æŸ¥/å¤åˆ¶
    if not getattr(tok, "chat_template", None):
        logger.warning(f"æ¨¡å‹ {args.model_path} ç¼ºå°‘ chat_templateï¼Œå°è¯•ä» Qwen3-8B å¤åˆ¶...")
        copied = False
        for cand in DEFAULT_QWEN_PATHS:
            try:
                qtok = AutoTokenizer.from_pretrained(
                    cand, use_fast=True, trust_remote_code=True,
                    local_files_only=_is_local(cand)
                )
                if getattr(qtok, "chat_template", None):
                    tok.chat_template = qtok.chat_template
                    logger.info(f"âœ“ å·²ä» {cand} å¤åˆ¶ chat_template")
                    copied = True
                    break
            except Exception as e:
                logger.warning(f"ä» {cand} å¤åˆ¶å¤±è´¥: {e}")
        if not copied:
            raise ValueError("æ— æ³•è·å– chat_templateï¼›è¯·ç¡®ä¿æœ¬åœ°æˆ–ç½‘ç»œå¯ç”¨çš„ Qwen3-8B å­˜åœ¨ã€‚")

    # enable_thinking æ¢æµ‹
    supports_thinking = False
    try:
        _ = tok.apply_chat_template([{"role":"user","content":"test"}],
                                    tokenize=False, add_generation_prompt=True, enable_thinking=True)
        supports_thinking = True
        logger.info("âœ“ Chat template æ”¯æŒ enable_thinking")
    except TypeError:
        logger.info("âš ï¸ Chat template ä¸æ”¯æŒ enable_thinking")
    enable_thinking_flag = args.enable_thinking and supports_thinking

    logger.info(f"âœ“ Tokenizer å°±ç»ª | è¯è¡¨: {len(tok)} | EOS: {tok.eos_token} | padding_side(train)={tok.padding_side}")

    # åŠ è½½æ¨¡å‹
    logger.info("åŠ è½½æ¨¡å‹...")
    quant_config = None
    if args.load_in_4bit or args.load_in_8bit:
        from transformers import BitsAndBytesConfig
        quant_config = BitsAndBytesConfig(
            load_in_4bit=args.load_in_4bit,
            load_in_8bit=args.load_in_8bit,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True
        )

    model = AutoModelForCausalLM.from_pretrained(
        args.model_path,
        torch_dtype=torch.bfloat16 if args.bf16 else "auto",
        device_map=None if args.deepspeed else "auto",
        trust_remote_code=True,
        quantization_config=quant_config,
        local_files_only=is_local_model
    )
    if quant_config is not None:
        model = prepare_model_for_kbit_training(model)

    # è¯è¡¨å¯¹é½
    if model.config.vocab_size != len(tok):
        logger.warning(f"vocab ä¸åŒ¹é…ï¼šmodel={model.config.vocab_size}, tok={len(tok)}ï¼Œè°ƒæ•´ embedding")
        model.resize_token_embeddings(len(tok))
    model.config.use_cache = False
    if model.config.pad_token_id is None:
        model.config.pad_token_id = tok.pad_token_id

    # LoRA æ³¨å…¥
    logger.info("æ³¨å…¥ LoRA...")
    target_modules = detect_lora_target_modules(model)
    logger.info(f"  LoRA ç›®æ ‡å±‚: {target_modules}")
    lora_config = LoraConfig(
        r=args.lora_r, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout,
        target_modules=target_modules, task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, lora_config)
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    logger.info(f"âœ“ LoRA æ³¨å…¥å®Œæˆ | å¯è®­ç»ƒå‚æ•°: {trainable:,}/{total:,} ({100*trainable/total:.2f}%)")

    # ç¼–è¯‘/SDPAï¼ˆå¯é€‰ï¼‰
    if args.enable_compile:
        model = try_enable_sdp_and_compile(model, logger)

    # æ•°æ®åŠ è½½
    logger.info("åŠ è½½æ•°æ®é›†...")
    train_raw = load_from_disk(args.train_dir)
    val_raw   = load_from_disk(args.val_dir)
    logger.info(f"åŸå§‹æ•°æ®: è®­ç»ƒ={len(train_raw)}, éªŒè¯={len(val_raw)}")

    _require_columns(train_raw, ["system","user","assistant","quality"], "train_raw")
    _require_columns(val_raw,   ["system","user"], "val_raw")

    # æ¸…æ´— + è¿‡æ»¤
    train_raw = train_raw.filter(_ok_train_row, num_proc=args.num_proc)
    val_raw   = val_raw.filter(_ok_eval_row,   num_proc=args.num_proc)
    logger.info(f"æ¸…æ´—å: è®­ç»ƒ={len(train_raw)}, éªŒè¯={len(val_raw)}")

    logger.info("è¿‡æ»¤è®­ç»ƒé›† (quality == 'gold')...")
    train_raw = train_raw.filter(lambda ex: ex.get("quality") == "gold", num_proc=args.num_proc)
    if len(train_raw) == 0:
        raise ValueError("è¿‡æ»¤åè®­ç»ƒé›†ä¸ºç©ºï¼è¯·æ£€æŸ¥æ•°æ®æˆ–å»æ‰ quality è¿‡æ»¤ã€‚")

    train_ds = train_raw.map(
        make_messages,
        remove_columns=[c for c in train_raw.column_names if c != "messages"],
        num_proc=args.num_proc
    )
    if args.max_train_samples and args.max_train_samples < len(train_ds):
        train_ds = train_ds.select(range(args.max_train_samples))
    val_ds = val_raw.map(
        keep_eval_fields,
        remove_columns=[c for c in val_raw.column_names if c not in ("id","system","user","assistant","answer")],
        num_proc=args.num_proc
    )
    if len(val_ds) == 0:
        raise ValueError("éªŒè¯é›†ä¸ºç©ºæˆ–ç¼ºå°‘å¿…è¦å­—æ®µï¼")

    logger.info(f"âœ“ æ•°æ®å‡†å¤‡å®Œæˆ | è®­ç»ƒ={len(train_ds)}, éªŒè¯={len(val_ds)}")

    # è®­ç»ƒé…ç½®
    max_len = min(args.cutoff_len, getattr(tok, "model_max_length", args.cutoff_len))
    formatting_func = create_formatting_func(tok, enable_thinking_default=enable_thinking_flag)
    sft_config = SFTConfig(
        output_dir=args.output_dir,
        learning_rate=args.lr,
        weight_decay=args.weight_decay,
        lr_scheduler_type="cosine",
        warmup_ratio=args.warmup_ratio,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.grad_accum,
        gradient_checkpointing=True,
        num_train_epochs=args.epochs,
        bf16=args.bf16,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        logging_first_step=True,
        report_to=["none"],
        max_length=max_len,
        packing=args.packing,
        deepspeed=args.deepspeed,
        max_grad_norm=1.0,  # ç¨³å®šè®­ç»ƒ
    )

    trainer = SFTTrainer(
        model=model,
        processing_class=tok,
        args=sft_config,
        train_dataset=train_ds,
        formatting_func=formatting_func,
    )

    # æŒ‡æ ‡å›è°ƒ
    metrics_cb = TrainingMetricsCallback(
        out_dir=os.path.join(args.output_dir, "metrics"),
        logger=logger,
        local_rank=local_rank
    )
    trainer.add_callback(metrics_cb)

    # æœ€ä¼˜ ckpt ç®¡ç†
    ckpt_cb = BestCheckpointCallback(
        output_dir=args.output_dir,
        save_total_limit=args.save_total_limit,
        metric_name="eval_accuracy",
        logger=logger,
        local_rank=local_rank
    )
    trainer.add_callback(ckpt_cb)

    # è¯„ä¼°å‡†å¤‡
    val_examples = [val_ds[i] for i in range(len(val_ds))]
    eval_batch_size = args.eval_batch_size if args.eval_batch_size is not None else args.per_device_train_batch_size
    logger.info(f"è¯„ä¼°æ‰¹æ¬¡å¤§å°: {eval_batch_size}")

    # æ„é€  eosï¼ˆä¸å« </answer>ï¼‰
    eos_ids: List[int] = []
    for tok_text in ("<|im_end|>", tok.eos_token):
        try:
            _id = tok.convert_tokens_to_ids(tok_text)
            if isinstance(_id, int) and _id != tok.unk_token_id:
                eos_ids.append(_id)
        except Exception:
            pass
    # å¯¹é½åˆ° generation_config
    gen_cfg = model.generation_config
    gen_cfg.pad_token_id = tok.pad_token_id
    if eos_ids:
        gen_cfg.eos_token_id = eos_ids if len(eos_ids) > 1 else eos_ids[0]

    gen_kwargs = {
        "max_new_tokens": 768,
        "do_sample": True,
        "temperature": 0.3,
        "top_p": 0.9,
        "repetition_penalty": 1.05,
    }
    if eos_ids:
        gen_kwargs["eos_token_id"] = eos_ids if len(eos_ids) > 1 else eos_ids[0]

    stop_tokens = args.extra_stop_tokens if args.extra_stop_tokens is not None else ["<|im_end|>"]

    eval_cb = EvalCallback(
        tokenizer=tok,
        val_examples=val_examples,
        out_dir=os.path.join(args.output_dir, "eval"),
        logger=logger,
        checkpoint_cb=ckpt_cb,
        local_rank=local_rank,
        max_eval_samples=args.max_eval_samples_arg,
        gen_kwargs=gen_kwargs,
        stop_tokens=stop_tokens,
        shuffle_eval_each_time=True,
        eval_batch_size=eval_batch_size,
    )
    trainer.add_callback(eval_cb)

    # åˆå§‹è¯„ä¼°
    if args.initial_eval and is_main_process(local_rank):
        logger.info("="*60); logger.info("åˆå§‹è¯„ä¼°ï¼ˆè®­ç»ƒå‰ï¼‰"); logger.info("="*60)
        eval_cb.run_eval(model, global_step=0)

    # è½ç›˜è¿è¡Œé…ç½®ä¸ tokenizer
    if is_main_process(local_rank):
        persist_run_config(args, args.output_dir)
        tok.save_pretrained(args.output_dir)

    # è®­ç»ƒ
    logger.info("="*60); logger.info("å¼€å§‹è®­ç»ƒ..."); logger.info("="*60)
    try:
        if args.resume_from_checkpoint:
            logger.info(f"ä» checkpoint æ¢å¤: {args.resume_from_checkpoint}")
            trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)
        else:
            trainer.train()
    except RuntimeError as e:
        if "CUDA out of memory" in str(e):
            logger.error("æ£€æµ‹åˆ° OOMï¼šè¯·é™ä½ batch æˆ–å¯ç”¨ 4/8bit / gradient_checkpointing / å‡å° cutoff_len")
        raise
    finally:
        if is_main_process(local_rank):
            try: trainer.save_state()
            except Exception: pass

    # æœ€ç»ˆè¯„ä¼°ä¸ä¿å­˜
    if is_main_process(local_rank):
        logger.info("="*60); logger.info("æœ€ç»ˆè¯„ä¼°..."); logger.info("="*60)
        eval_cb.run_eval(model, global_step=-1)
        logger.info("="*60); logger.info("ä¿å­˜æ¨¡å‹ä¸ tokenizer..."); logger.info("="*60)
        trainer.save_model(args.output_dir)
        tok.save_pretrained(args.output_dir)
        try:
            if hasattr(model, "save_pretrained"):
                model.save_pretrained(os.path.join(args.output_dir, "adapter"))
        except Exception:
            pass
        logger.info("è®­ç»ƒå®Œæˆ âœ…")

if __name__ == "__main__":
    main()
