import os
import argparse
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed

def main():
    parser = argparse.ArgumentParser(description="æ£€æŸ¥å¹¶æ·»åŠ ç‰¹æ®Štokenåˆ°æ¨¡å‹è¯è¡¨")
    parser.add_argument("--model_path", type=str, required=True, help="åŸå§‹æ¨¡å‹è·¯å¾„")
    parser.add_argument("--save_path", type=str, required=True, help="ä¿å­˜æ–°æ¨¡å‹è·¯å¾„")
    parser.add_argument("--special_tokens", type=str, nargs="+", required=True, help="è¦æ·»åŠ çš„ç‰¹æ®Štokenåˆ—è¡¨")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)

    print("=" * 80)
    print("ğŸš€ å¼€å§‹å¤„ç†æ¨¡å‹ç‰¹æ®Š token")
    print("=" * 80)
    print(f"ğŸ“‚ åŸå§‹æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"ğŸ’¾ ä¿å­˜è·¯å¾„: {args.save_path}")
    print(f"ğŸ”¤ å¾…æ£€æŸ¥çš„ç‰¹æ®Š token: {args.special_tokens}")
    print(f"ğŸ² éšæœºç§å­: {args.seed}")
    print("=" * 80)

    print(f"\nğŸ”¹ åŠ è½½æ¨¡å‹å’Œ tokenizerï¼š{args.model_path}")
    model = AutoModelForCausalLM.from_pretrained(args.model_path, torch_dtype=torch.bfloat16)
    tokenizer = AutoTokenizer.from_pretrained(args.model_path)
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"ğŸ“Š å½“å‰è¯è¡¨å¤§å°: {len(tokenizer)}")
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {model.num_parameters():,}")

    # æ£€æŸ¥å“ªäº› token ç¼ºå¤±
    print("\n" + "=" * 80)
    print("ğŸ” æ£€æŸ¥ç‰¹æ®Š token æ˜¯å¦å­˜åœ¨äºè¯è¡¨ä¸­...")
    print("=" * 80)
    missing_tokens = []
    existing_tokens = []
    for tok in args.special_tokens:
        tok_id = tokenizer.convert_tokens_to_ids(tok)
        if tok_id == tokenizer.unk_token_id:
            print(f"âŒ æœªæ‰¾åˆ°ï¼š{tok} (å°†è¢«æ·»åŠ )")
            missing_tokens.append(tok)
        else:
            print(f"âœ… å·²å­˜åœ¨ï¼š{tok} (ID={tok_id})")
            existing_tokens.append((tok, tok_id))

    print("\n" + "-" * 80)
    print(f"ğŸ“ˆ ç»Ÿè®¡ç»“æœ:")
    print(f"   âœ… å·²å­˜åœ¨çš„ token æ•°é‡: {len(existing_tokens)}")
    print(f"   âŒ ç¼ºå¤±çš„ token æ•°é‡: {len(missing_tokens)}")
    print("-" * 80)

    # è‹¥æœ‰ç¼ºå¤±åˆ™æ·»åŠ 
    if missing_tokens:
        print(f"\nğŸš€ å¼€å§‹æ·»åŠ æ–°çš„ç‰¹æ®Š tokenï¼š{missing_tokens}")
        original_vocab_size = len(tokenizer)
        tokenizer.add_special_tokens({'additional_special_tokens': missing_tokens})
        new_vocab_size = len(tokenizer)
        print(f"ğŸ“Š è¯è¡¨å¤§å°å˜åŒ–: {original_vocab_size} â†’ {new_vocab_size} (+{new_vocab_size - original_vocab_size})")
        
        print(f"ğŸ”§ è°ƒæ•´æ¨¡å‹ embedding å±‚å¤§å°...")
        model.resize_token_embeddings(len(tokenizer))
        print(f"âœ… Embedding å±‚è°ƒæ•´å®Œæˆ")
        
        # éªŒè¯æ–°æ·»åŠ çš„ token
        print(f"\nğŸ” éªŒè¯æ–°æ·»åŠ çš„ token:")
        for tok in missing_tokens:
            tok_id = tokenizer.convert_tokens_to_ids(tok)
            print(f"   âœ… {tok} â†’ ID={tok_id}")
    else:
        print("\nâœ… æ— éœ€æ·»åŠ æ–° tokenï¼Œè¯è¡¨å·²å®Œæ•´ã€‚")

    # ä¿å­˜æ‰©å±•åçš„æ¨¡å‹ä¸ tokenizer
    print("\n" + "=" * 80)
    print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° {args.save_path}")
    print("=" * 80)
    os.makedirs(args.save_path, exist_ok=True)
    
    print("ğŸ’¾ ä¿å­˜ tokenizer...")
    tokenizer.save_pretrained(args.save_path)
    print("âœ… Tokenizer ä¿å­˜å®Œæˆ")
    
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
    model.save_pretrained(args.save_path)
    print("âœ… æ¨¡å‹ä¿å­˜å®Œæˆ")

    print("\n" + "=" * 80)
    print("ğŸ‰ å¤„ç†å®Œæˆï¼")
    print("=" * 80)
    print(f"ğŸ“‚ åŸå§‹æ¨¡å‹: {args.model_path}")
    print(f"ğŸ’¾ æ–°æ¨¡å‹ä¿å­˜åˆ°: {args.save_path}")
    print(f"ğŸ“Š æœ€ç»ˆè¯è¡¨å¤§å°: {len(tokenizer)}")
    print(f"ğŸ“Š ç‰¹æ®Š token æ€»æ•°: {len(args.special_tokens)}")
    print(f"   - åŸæœ¬å­˜åœ¨: {len(existing_tokens)}")
    print(f"   - æ–°å¢åŠ çš„: {len(missing_tokens)}")
    print("=" * 80)

if __name__ == "__main__":
    main()
